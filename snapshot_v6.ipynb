{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "snapshot_v6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henriChab/snapshot/blob/master/snapshot_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by1-FkJb88uE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHeX-g4pkNwD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a5e9ab36-1e25-46b3-d073-24ce3fa103e2"
      },
      "source": [
        "!pip install premailer\n",
        "!pip install openpyxl\n",
        "!pip install pyarrow\n",
        "!pip install pypardot4\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: premailer in /usr/local/lib/python3.6/dist-packages (3.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from premailer) (2.21.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from premailer) (4.2.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.6/dist-packages (from premailer) (3.1.1)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.6/dist-packages (from premailer) (1.0.3)\n",
            "Requirement already satisfied: cssutils in /usr/local/lib/python3.6/dist-packages (from premailer) (1.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->premailer) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->premailer) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->premailer) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->premailer) (2.8)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.6/dist-packages (2.5.9)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.6/dist-packages (from openpyxl) (1.0.1)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.6/dist-packages (from openpyxl) (1.4.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.16.4)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.12.0)\n",
            "Requirement already satisfied: pypardot4 in /usr/local/lib/python3.6/dist-packages (1.1.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pypardot4) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pypardot4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pypardot4) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pypardot4) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pypardot4) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQN3ByrwCPDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rfN3nToj90P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as tick\n",
        "import matplotlib\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import boto3\n",
        "import copy as cp\n",
        "import re\n",
        "import io\n",
        "import sys\n",
        "import premailer\n",
        "import openpyxl\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "from string import Formatter\n",
        "from pydoc import locate\n",
        "import itertools\n",
        "import pyarrow\n",
        "\n",
        "S3_BUCKET_CONF = \"adomik-maths-preproduction\"\n",
        "S3_PREFIX_CONF = \"henri/snapshot/\"\n",
        "\n",
        "S3_GDX_LOG = \"adomik-production-logs-gdx\"\n",
        "S3_SELL_LOG = \"adomik-pipe-output-prod\"\n",
        "S3_SELL_PREFIX = \"data-pipeline-outputs/tmp/Daily/UNF/\"\n",
        "\n",
        "COLUMN_CONF_KEY = S3_PREFIX_CONF+\"conf/column_labelling_dev.json\"\n",
        "CLIENTS_FILE_KEY = S3_PREFIX_CONF+\"conf/clients.json\"\n",
        "PRODUCT_FILE_KEY = S3_PREFIX_CONF+\"conf/products.json\"\n",
        "CALCULATED_FIELDS_KEY = S3_PREFIX_CONF+\"conf/calculated_metrics.json\"\n",
        "PROGRAM_CONF_KEY = S3_PREFIX_CONF+\"conf/programs_conf.json\"\n",
        "\n",
        "\n",
        "\n",
        "AWS_ID = \"AKIAJEWDBGDATNFEBDSA\"\n",
        "AWS_SECRET = \"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\"\n",
        "\n",
        "GMAIL_SERVER=\"smtp.gmail.com\"\n",
        "GMAIL_LOGIN=\"report@adomik.com\"\n",
        "GMAIL_PASSWD=\"Adom!krep0rt123\"\n",
        "GMAIL_PORT=587\n",
        "\n",
        "PARDOT_EMAIL=\"henri@adomik.com\"\n",
        "PARDOT_PASSWD=\"Tionospeck,46\"\n",
        "PARDOT_API_KEY=\"528c7951733d685e4d1acc66ddad8672\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_RDdGeTj90S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Client():\n",
        "    \"\"\"\n",
        "    Object to avoid defining all the time the s3 client. Once it is defined, you can use Client.client to access the s3 client.\n",
        "\n",
        "    Args:\n",
        "        aws_id (string): The public key of the AWS account\n",
        "        aws_secret (string): The private key of the AWS account\n",
        "    \"\"\"\n",
        "    def __init__(self, aws_id, aws_secret):\n",
        "        self.aws_id = aws_id\n",
        "        self.aws_secret = aws_secret\n",
        "        self.client = boto3.client('s3', aws_access_key_id=aws_id, aws_secret_access_key=aws_secret)\n",
        "        self.resource = boto3.resource('s3')\n",
        "\n",
        "cli = Client(aws_id=AWS_ID, aws_secret=AWS_SECRET) #We instanciate the client\n",
        "\n",
        "def mem_usage(pandas_obj):\n",
        "    if isinstance(pandas_obj,pd.DataFrame):\n",
        "      usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
        "    else: # we assume if not a df it's a series\n",
        "      usage_b = pandas_obj.memory_usage(deep=True)\n",
        "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
        "    return \"{:03.2f} MB\".format(usage_mb)\n",
        "\n",
        "def optimize_df_int(df):\n",
        "    optimized_df = df.copy()\n",
        "    df_int = optimized_df.select_dtypes(include=['int'])\n",
        "    converted_int = df_int.apply(pd.to_numeric, downcast='unsigned')\n",
        "    optimized_df[converted_int.columns] = converted_int\n",
        "    return(optimized_df)\n",
        "  \n",
        "def optimize_df_float(df):\n",
        "    optimized_df = df.copy()\n",
        "    df_float = optimized_df.select_dtypes(include=['float'])\n",
        "    converted_float = df_float.apply(pd.to_numeric, downcast='float')\n",
        "    optimized_df[converted_float.columns] = converted_float\n",
        "    return(optimized_df)\n",
        "\n",
        "def convert_categories_to_str(df):\n",
        "    unoptimized_df = df.copy()\n",
        "    if isinstance(df, pd.DataFrame):\n",
        "      df_cat = unoptimized_df.select_dtypes(include=['category'])\n",
        "      converted_cat = pd.DataFrame()\n",
        "      for col in df_cat.columns:\n",
        "        converted_cat.loc[:,col] = df_cat[col].astype('object')\n",
        "\n",
        "      unoptimized_df[converted_cat.columns] = converted_cat\n",
        "    else:\n",
        "      if str(unoptimized_df.dtype) == \"category\":\n",
        "        unoptimized_df = unoptimized_df.astype('object')\n",
        "      else:\n",
        "        unoptimized_df.dtype\n",
        "    return(unoptimized_df)\n",
        "  \n",
        "def convert_str_to_categories(df):\n",
        "    optimized_df = df.copy()\n",
        "    df_obj = optimized_df.select_dtypes(include=['object'])\n",
        "    converted_obj = pd.DataFrame()\n",
        "    for col in df_obj.columns:\n",
        "      num_unique_values = len(df_obj[col].unique())\n",
        "      num_total_values = len(df_obj[col])\n",
        "      if num_unique_values / num_total_values < 0.5:\n",
        "        converted_obj.loc[:,col] = df_obj[col].astype('category')\n",
        "      else:\n",
        "        converted_obj.loc[:,col] = df_obj[col]\n",
        "     \n",
        "    optimized_df[converted_obj.columns] = converted_obj\n",
        "    return(optimized_df)\n",
        "  \n",
        "def optimize_df(df):\n",
        "  print(\"Start df opitmization\")\n",
        "  optimized_df = convert_str_to_categories(df)\n",
        "  optimized_df = optimize_df_float(optimized_df)\n",
        "  optimized_df = optimize_df_int(optimized_df)\n",
        "  print(\"End df opitmization\")\n",
        "  return(optimized_df)\n",
        "\n",
        "def unoptimize_df(df):\n",
        "  print(\"Start df unopitmization\")\n",
        "  unoptimized_df = convert_categories_to_str(df)\n",
        "  print(\"End df unopitmization\")\n",
        "  return(unoptimized_df)\n",
        "\n",
        "\n",
        "def timeit(method):\n",
        "    def timed(*args, **kw):\n",
        "        ts = time.time()\n",
        "        result = method(*args, **kw)\n",
        "        te = time.time()\n",
        "        if 'log_time' in kw:\n",
        "            name = kw.get('log_name', method.__name__.upper())\n",
        "            kw['log_time'][name] = int((te - ts) * 1000)\n",
        "        else:\n",
        "            print('%r  %2.2f ms' % \\\n",
        "                  (method.__name__, (te - ts) * 1000))\n",
        "        return(result)\n",
        "    return(timed)\n",
        "  \n",
        "  \n",
        "def get_s3_file(bucket, keys, local=False, add_concat_column=[], sep=\"\\t\", header=0):\n",
        "    \"\"\"\n",
        "    Get a file from s3. Format it if file is csv or json.\n",
        "\n",
        "    Args:\n",
        "        bucket (string): The s3 bucket of the object(s)\n",
        "        keys (list[string], tuple(string), string): The key(s) of the objects to retrieve\n",
        "        file_format (string): The format of the file\n",
        "        local (boolean): True if you want to concatenate all the objects. If False, return a list of them (Default value = False)\n",
        "        concat_df (boolean):  (Default value = True)\n",
        "        add_concat_column (list): If concat_df, create column(s) named as 'new_col_name'\n",
        "            and having as values the element of the key situated at position filename_ind (Default value = [])\n",
        "            add_concat_column with the shape :\n",
        "                [\n",
        "                    {\n",
        "                        filename_ind:int,\n",
        "                        new_col_name:str\n",
        "                    },\n",
        "                    ...\n",
        "                ]\n",
        "\n",
        "    Returns:\n",
        "        (list/DataFrame/dict/str) The file from s3\n",
        "    \"\"\"\n",
        "    is_str = False\n",
        "    if isinstance(keys, str):\n",
        "        is_str = True\n",
        "        keys = [keys]\n",
        "    if isinstance(keys, (list, tuple)):\n",
        "        file_format = keys[0].split(\"/\")[-1].split(\".\")[-1]\n",
        "        if is_concatenable(file_format):\n",
        "          file_result = pd.DataFrame()\n",
        "        else:\n",
        "          file_result = []\n",
        "        for i, k in enumerate(keys):\n",
        "            #progressBar(i+1, len(keys), bar_length=100)\n",
        "            if local:\n",
        "                if file_format == \"json\":\n",
        "                    with open(bucket+\"/\"+k, 'r') as outfile:\n",
        "                        file_result.append(json.loads(outfile))\n",
        "                elif file_format == \"csv\":\n",
        "                    new_df = pd.read_csv(bucket+\"/\"+k, error_bad_lines=False, sep=sep, header=header, warn_bad_lines=False)\n",
        "                elif file_format == \"parquet\":\n",
        "                    new_df = pd.read_parquet(bucket+\"/\"+k)\n",
        "            else:\n",
        "                file_content = cli.client.get_object(Bucket=bucket, Key=k)['Body'].read()\n",
        "                if file_format == \"json\":\n",
        "                    file_result.append(json.loads(file_content))\n",
        "                elif file_format == \"csv\":\n",
        "                    new_df = pd.read_csv(io.BytesIO(file_content), error_bad_lines=False, sep=sep, header=header, warn_bad_lines=False)\n",
        "                elif file_format == \"parquet\":\n",
        "                    new_df = pd.read_parquet(io.BytesIO(file_content))\n",
        "                else:\n",
        "                    file_result.append(file_content.decode('utf-8'))\n",
        "                    \n",
        "                \n",
        "                for cl in add_concat_column:\n",
        "                    try:\n",
        "                      new_df[cl[\"new_col_name\"]] = k.split(\"/\")[cl[\"filename_ind\"]]\n",
        "                    except:\n",
        "                        print(\"The shape of 'add_concat_column' is not good.\")\n",
        "                \n",
        "            if is_concatenable(file_format):\n",
        "                file_result = pd.concat([file_result, new_df])\n",
        "            \n",
        "        if is_str and not is_concatenable(file_format):\n",
        "          return(file_result[0])\n",
        "        else:\n",
        "          return(file_result)\n",
        "    print(\"The argument 'keys' must be str, list or tuple\")\n",
        "    return None\n",
        "\n",
        "def is_concatenable(file_format):\n",
        "    concatenable_formats = [\"csv\", \"parquet\"]\n",
        "    if file_format in concatenable_formats:\n",
        "        return (True)\n",
        "    return(False)\n",
        "\n",
        "def existing_path(bucket, key, ):\n",
        "    \"\"\"\n",
        "    Check if a path exists in s3\n",
        "\n",
        "    Args:\n",
        "        bucket (string): Name of the s3 bucket\n",
        "        key (string): Name of the s3 key\n",
        "\n",
        "    Returns:\n",
        "        (boolean) True if the path exists, False else\n",
        "    \"\"\"\n",
        "    existing_keys = list(get_matching_s3_keys(\n",
        "        bucket=bucket,\n",
        "        prefix=key\n",
        "    ))\n",
        "    if existing_keys:\n",
        "        return(True)\n",
        "    return(False)\n",
        "    \n",
        "def show_col(bucket, key, sep=\"\\t\"):\n",
        "    \"\"\"\n",
        "    Display the columns of a csv file on s3\n",
        "\n",
        "    Args:\n",
        "        bucket (string): Name of the s3 bucket\n",
        "        key (string): Name of the s3 key \n",
        "        sep (string): The csv separator (Default value = \"\\t\")\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    if key.split(\".\")[1] == \"csv\":\n",
        "        obj = cli.client.get_object(Bucket=bucket, Key=key)['Body']\n",
        "        csv_string = obj.next()\n",
        "        df = pd.read_csv(io.StringIO(csv_string), error_bad_lines=False, sep=sep)\n",
        "        print(df.columns)\n",
        "        obj.close()\n",
        "    else:\n",
        "        print(\"Only csv can be used.\")\n",
        "\n",
        "def is_list_of_type(l, l_type):\n",
        "    \"\"\"\n",
        "    Check if the list only contains \n",
        "\n",
        "    Args:\n",
        "        l (list[]): The list to check\n",
        "        l_type (type): The instance_type\n",
        "\n",
        "    Returns:\n",
        "        (boolean) True if the list only contains elements of the type l_type\n",
        "    \"\"\"\n",
        "    for e in l:\n",
        "        if not isinstance(e,l_type):\n",
        "            return(False)\n",
        "    return(True)\n",
        "\n",
        "def create_random_df(n, m):\n",
        "    df = pd.DataFrame(np.random.randint(0,100,size=(n, m)), columns=[\"c_\"+str(i) for i in range(m)])\n",
        "    return(df)\n",
        "\n",
        "\n",
        "def listToDateTime(l, formatter=\"%Y-%m-%d\"):\n",
        "    \"\"\"\n",
        "    Converts a list of string to datetime\n",
        "\n",
        "    Args:\n",
        "        l (list[string]): The list to convert\n",
        "        formatter (string): The format of the string representing a date\n",
        "\n",
        "    Returns:\n",
        "        (list[datetime]) The formatted list of datetime\n",
        "    \"\"\"\n",
        "    \n",
        "    new_l = list(map(lambda x : datetime.datetime.strptime(x, formatter), l))\n",
        "\n",
        "def dateTimeToList(liste, formatter=\"%Y-%m-%d\"):\n",
        "    \"\"\"\n",
        "    Converts a list of datetime to string\n",
        "\n",
        "    Args:\n",
        "        l (list[datetime]): The list to convert\n",
        "        formatter (string): The way to format output dates\n",
        "\n",
        "    Returns:\n",
        "        (list[string]) The formatted list of String\n",
        "    \"\"\"\n",
        "    return(list(map(lambda x : x.strftime(formatter), liste)))\n",
        "\n",
        "def strToDateTime(date_str, formatter=\"%Y-%m-%d\"): #Transforms a String to DateTime\n",
        "    \"\"\"\n",
        "    Converts a string to datetime\n",
        "\n",
        "    Args:\n",
        "        date_str (string): The string to convert\n",
        "        formatter (string): The format of the string representing a date\n",
        "\n",
        "    Returns:\n",
        "        (datetime) The formatted datetime\n",
        "    \"\"\"\n",
        "    return(datetime.datetime.strptime(date_str, formatter))\n",
        "\n",
        "def dateTimeToStr(dt, formatter=\"%Y-%m-%d\"): #Transforms a DateTime to String\n",
        "    \"\"\"\"\n",
        "    Converts a datetime to String\n",
        "\n",
        "    Args:\n",
        "        dt (datetime): The datetime to convert\n",
        "        formatter (string): The way to format output date\n",
        "\n",
        "    Returns:\n",
        "        (string) The formatted string\n",
        "    \"\"\"\n",
        "    return(dt.strftime(formatter))\n",
        "    \n",
        "def is_l1_in_l2(l2, l1):\n",
        "    \"\"\"\n",
        "    Checks if list l2 contains list l1\n",
        "\n",
        "    Args:\n",
        "        l2 (list[]): List in which to search the sublist\n",
        "        l1 (list[]): Sublist\n",
        "\n",
        "    Returns:\n",
        "        (boolean) True if l1 is in l2\n",
        "    \"\"\"\n",
        "    for el in l1:\n",
        "        if not el in l2:\n",
        "            return (False)\n",
        "    return(True)\n",
        "    \n",
        "def get_timestamp(formatter=\"%H-%M-%S-%f\"):\n",
        "    \"\"\"\n",
        "    Returns the current timestamp\n",
        "\n",
        "    Args:\n",
        "        formatter (string): The way to format the timestamp  \n",
        "\n",
        "    Returns:\n",
        "        (string) The current timestamp\n",
        "    \"\"\"\n",
        "    return(str(datetime.datetime.now().strftime(formatter)))\n",
        "\n",
        "def get_fct_args(fct):\n",
        "    all_args = fct.__code__.co_varnames[:fct.__code__.co_argcount]\n",
        "    mandatory_args = all_args[:len(all_args) - len(fct.__defaults__)]\n",
        "    optionnal_args = all_args[len(all_args) - len(fct.__defaults__):]\n",
        "    \n",
        "    return([mandatory_args, optionnal_args])\n",
        "\n",
        "def col_cast(df, caster):\n",
        "    \"\"\"\n",
        "    Cast the columns of a DataFrame\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame):  The DataFrame for which we want to convert the type of the columns\n",
        "        caster (dict): The columns to cast with their type \n",
        "            Ex: {\"col1\":\"type1\", \"col2\":\"type2\", ...} \n",
        "\n",
        "    Returns:\n",
        "        (DateFrame) the DataFrame with casted columns\n",
        "    \"\"\"\n",
        "    formated_caster = {}\n",
        "    for i, col in enumerate(caster):\n",
        "        progressBar(i, len(caster), bar_length=100)\n",
        "        if caster[col] == \"date\":\n",
        "            df[col] = pd.to_datetime(df[col])\n",
        "        else:\n",
        "            real_type = caster[col]\n",
        "            if caster[col] == \"amount\" or caster[col] == \"perc\":\n",
        "                real_type = \"float64\"\n",
        "            elif caster[col] == \"string\":\n",
        "                real_type = \"str\"\n",
        "            formated_caster[col] = locate(real_type)\n",
        "    return(df.astype(formated_caster))\n",
        "    \n",
        "def contains_list(el, l):\n",
        "    \"\"\"\n",
        "    Checks if one of the element of the list contains the substring el\n",
        "\n",
        "    Args:\n",
        "        el (string): The substring\n",
        "        l (list[list,string]): The list for which to check if at least one of the elements contains the substring\n",
        "\n",
        "    Returns:\n",
        "        (boolean) True if at least one of the elements of l contains el\n",
        "    \"\"\"\n",
        "    for e in l:\n",
        "        if e in el:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def build_dates_list(sdate, edate):\n",
        "    \"\"\"\n",
        "    Build a list of datetimes between sdate and edate\n",
        "\n",
        "    Args:\n",
        "      sdate (datetime): Start date from which the dates will be generated \n",
        "      edate (datetime): End date \n",
        "\n",
        "    Returns:\n",
        "        (list[datetime]) The datetime between sdate and edate\n",
        "    \"\"\"\n",
        "    dates_list = []\n",
        "    delta = edate - sdate\n",
        "    for i in range(delta.days + 1):\n",
        "        dates_list.append(sdate + datetime.timedelta(i))\n",
        "    return(dates_list)\n",
        "\n",
        "def cartesian_product(key_to_format, replace_params):\n",
        "    product_list = []\n",
        "    for tup in itertools.product(*[[{d:i} for i in replace_params[d]] if isinstance(replace_params[d], list) else [{d:replace_params[d]}] for d in replace_params]):\n",
        "        new_key = key_to_format.format(**{k:e[k] for e in tup for k in e})\n",
        "        product_list.append(new_key)\n",
        "    return(product_list)\n",
        "\n",
        "def get_matching_s3_keys(bucket, prefix='', suffix='', contains=''):\n",
        "    \"\"\"\n",
        "    Generate the keys in an S3 bucket.\n",
        "\n",
        "    :param bucket: Name of the S3 bucket.\n",
        "    :param prefix: Only fetch keys that start with this prefix (optional).\n",
        "    :param suffix: Only fetch keys that end with this suffix (optional).\n",
        "    \"\"\"\n",
        "    kwargs = {'Bucket': bucket}\n",
        "\n",
        "    # If the prefix is a single string (not a tuple of strings), we can\n",
        "    # do the filtering directly in the S3 API.\n",
        "    if isinstance(prefix, str):\n",
        "        kwargs['Prefix'] = prefix\n",
        "\n",
        "    while True:\n",
        "\n",
        "        # The S3 API response is a large blob of metadata.\n",
        "        # 'Contents' contains information about the listed objects.\n",
        "        resp = cli.client.list_objects_v2(**kwargs)\n",
        "        if 'Contents' in resp:\n",
        "            for obj in resp['Contents']:\n",
        "                key = obj['Key']\n",
        "                if key.startswith(prefix) and key.endswith(suffix) and contains in key:\n",
        "                    yield key\n",
        "\n",
        "        # The S3 API is paginated, returning up to 1000 keys at a time.\n",
        "        # Pass the continuation token into the next response, until we\n",
        "        # reach the final page (when this field is missing).\n",
        "        try:\n",
        "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
        "        except KeyError:\n",
        "            break\n",
        "            \n",
        "def progressBar(value, endvalue, bar_length=20): #Handle the progressbar incrementation\n",
        "    percent = float(value) / endvalue\n",
        "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
        "    spaces = ' ' * (bar_length - len(arrow))\n",
        "\n",
        "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(arrow + spaces, int(round(percent * 100))))\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "def build_dir_list(bucket, key_to_format, replace_params, verify_if_exists=True):\n",
        "    \"\"\"\n",
        "    Build a list by replacing the character substr of s3 key_to_format by all the elements of list_of_str\n",
        "\n",
        "    Args:\n",
        "        bucket (string): S3 bucket of the objects\n",
        "        key_to_format (string): The key containing the substr parameter which will be replaced.\n",
        "        list_of_str (list[string]): The list of elements that will replace substr.\n",
        "        substr (string): The substring that will be replaced by the elements of list_of_str\n",
        "\n",
        "    Returns:\n",
        "        (list[string]) List of key_to_format for which the substr has been replaced by each of the elements of list_of_str\n",
        "    \"\"\"\n",
        "    existing_dir_list = []\n",
        "    dir_list = cartesian_product(key_to_format, replace_params)\n",
        "    for dire in dir_list:\n",
        "        if verify_if_exists:\n",
        "            if existing_path(bucket, dire):\n",
        "                existing_dir_list.append(dire)\n",
        "        else:\n",
        "            existing_dir_list.append(dire)\n",
        "    return(existing_dir_list)\n",
        "\n",
        "def get_products_filter_tuple(products, channels_to_filter):\n",
        "    channels_str = \",\".join([\"\\\"\"+product.channel+\"_\"+chann+\"\\\"\" for chann in channels_to_filter for product in products])\n",
        "    return(channels_str)\n",
        "\n",
        "def get_products_channels(products):\n",
        "    product_channels = []\n",
        "    if isinstance(products, str):\n",
        "        products = [products]\n",
        "    for product in products:\n",
        "        if product == \"inapp\":\n",
        "            product_channels.append(\"ada\")\n",
        "        elif product == \"game\":\n",
        "            product_channels.append(\"adg\")\n",
        "        elif product == \"AMP\":\n",
        "            product_channels.append(\"ad_amp_group=ad\")\n",
        "        else:\n",
        "            product_channels.append(\"ad\")\n",
        "    return(product_channels)\n",
        "    \n",
        "def check_parameters(parameters, default_values):\n",
        "    \"\"\"\n",
        "    Assign default parameters to the dict parameters if the key is not found in it\n",
        "\n",
        "    Args:\n",
        "        parameters (dict): Dict of parameters to check\n",
        "        default_values (dict):  Dict of default parameters to assign if the key doesn't exists in parameters\n",
        "\n",
        "    Returns:\n",
        "        (dict) Checked dict containing all the keys of default_values and the values of parameters or default_values\n",
        "    \"\"\"\n",
        "    new_params = parameters\n",
        "    for param in default_values:\n",
        "        if default_values[param] == \"mandatory\" and not param in parameters:\n",
        "            print(\"You have not defined the parameter : \"+str(param)+\". Please add it to the parameters then create the object again.\")\n",
        "            return None\n",
        "        else:\n",
        "            if not param in parameters:\n",
        "                new_params[param] = default_values[param]\n",
        "    return(new_params)\n",
        "    \n",
        "def get_s3_url(bucket, key, timeout=100000000):\n",
        "    \"\"\"\n",
        "    Generate an url to access a file externaly\n",
        "\n",
        "    Args:\n",
        "        bucket (string): The bucket of the file\n",
        "        key (string): The key of the file\n",
        "        timeout (int): The time the link will be valid (in second) (Default value = 100000000)\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        url = cli.client.generate_presigned_url('get_object', Params={'Bucket': bucket, 'Key': key}, ExpiresIn=timeout)\n",
        "    except:\n",
        "        print(\"File {file_key} not found in bucket {bucket}\".format(\n",
        "            file_key=key,\n",
        "            bucket=bucket\n",
        "        ))\n",
        "        url = \"\"\n",
        "    return(url)\n",
        "    \n",
        "def export_excel(df):\n",
        "    \"\"\"\n",
        "    Convert a Pandas DataFrame into an Excel file\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): The DataFrame to convert\n",
        "\n",
        "    Returns:\n",
        "        (ExcelFile) The Excel File\n",
        "    \"\"\"\n",
        "    with io.BytesIO() as buffer:\n",
        "        writer = pd.ExcelWriter(buffer)\n",
        "        df.to_excel(writer)\n",
        "        writer.save()\n",
        "        return buffer.getvalue()\n",
        "    \n",
        "def get_object_from_object_list(obj_list, key, value):\n",
        "    \"\"\"\n",
        "    From the list of Seller, get a seller according to a att and a value\n",
        "\n",
        "    Args:\n",
        "        seller_list (list[Seller]): The list in which to search the seller(s) having att=value\n",
        "        att (string): The attribut to search\n",
        "        value (string): The value of the attribut\n",
        "\n",
        "    Returns:\n",
        "        (list[Seller]) The Seller(s) having att=value\n",
        "    \"\"\"\n",
        "    return([o for o in obj_list if getattr(o, key) == value])\n",
        "\n",
        "def recalculate_revenue(df, threshold=60):\n",
        "    \"\"\"\n",
        "    Recalculate revenue, because DFP reports only contain two figures after the decimal point\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): The DataFrame for which to recalculate the column \"estimated_revenue\"\n",
        "        threshold (int): The threshold beyond which the revenue will not be recalculated (Default value = 60)\n",
        "\n",
        "    Returns:\n",
        "        (DataFrame) The DataFrame containing the recalculated revenue\n",
        "    \"\"\"\n",
        "\n",
        "    if 'ad_e_cpm' not in df.columns or \"estimated_revenue\" not in df.columns or \"imps\" not in df.columns:\n",
        "        print('You must have Ad eCPM, Estimated revenue and Imps in columns. Not recalculating revenue')\n",
        "    else:\n",
        "        mask = (df['estimated_revenue'] < threshold)\n",
        "        df['revenue'] = df['estimated_revenue']      \n",
        "        df.loc[mask, 'revenue'] = df[mask]['imps']*df[mask]['ad_e_cpm']*0.001                  \n",
        "        df = df.drop('estimated_revenue', axis=1)     \n",
        "    return (df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def start_gmail_server():\n",
        "    \"\"\"\n",
        "    Starts the Gmail server\n",
        "\n",
        "    Returns:\n",
        "        (Server) The Gmail server to use to send the email(s)\n",
        "\n",
        "    \"\"\"\n",
        "    smtp_serv = SMTPServer(host=os.environ['GMAIL_SERVER'], login=os.environ['GMAIL_LOGIN'], password=os.environ['GMAIL_PASSWD'], port=os.environ['GMAIL_PORT'])\n",
        "    cont = ssl.create_default_context()\n",
        "    try:\n",
        "        serv = smtplib.SMTP(smtp_serv.host, smtp_serv.port)\n",
        "        #serv.set_debuglevel(1)\n",
        "        serv.ehlo()\n",
        "        serv.starttls()\n",
        "        serv.ehlo()\n",
        "    except:\n",
        "        coderr = \"%s\" % sys.exc_info()[1]\n",
        "        print(\"Connexion problem (\" + coderr + \")\")\n",
        "        return\n",
        "    if smtp_serv.login != \"\":\n",
        "        try:\n",
        "            serv.login(smtp_serv.login, smtp_serv.password)\n",
        "        except:\n",
        "            coderr = \"%s\" % sys.exc_info()[1]\n",
        "            serv.quit()\n",
        "            print(\"Wrong credentials (\" + coderr + \")\")\n",
        "            return\n",
        "    print(\"Connexion successful\")\n",
        "    return(serv)\n",
        "\n",
        "def get_formatable_fields(string):\n",
        "    fieldnames = [fname for _, fname, _, _ in Formatter().parse(string) if fname]\n",
        "    fieldnames = [f.split(\".\")[0] if \".\" in f else f for f in fieldnames]\n",
        "    return(fieldnames)\n",
        "\n",
        "def get_adk_channels_filter_from_obj(obj):\n",
        "    if obj.get_parameter(\"product\"):\n",
        "        return(\n",
        "            \",\".join(\n",
        "                cartesian_product(\n",
        "                    key_to_format=\"'{product.channel}_{channel}'\",\n",
        "                    replace_params={\n",
        "                        \"product\":obj.get_parameter(\"product\"),\n",
        "                        \"channel\":[\"opt\", \"ex0\", \"ex1\", \"ex2\", \"ex3\", \"ex4\", \"ex5\", \"ex6\", \"ex7\", \"ex8\", \"bc\"]\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    return(\"\")\n",
        "\n",
        "def get_bc_channels_filter_from_obj(obj):\n",
        "    if obj.get_parameter(\"product\"):\n",
        "        return(\n",
        "            \",\".join(\n",
        "                cartesian_product(\n",
        "                    key_to_format=\"'{product.channel}_{channel}'\",\n",
        "                    replace_params={\n",
        "                        \"product\":obj.get_parameter(\"product\"),\n",
        "                        \"channel\":[\"bc\"]\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    return(\"\")\n",
        "\n",
        "def format_to_universal_metrics(obj, string, extra_parameters={}):\n",
        "    \"\"\"\n",
        "    Send the email\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    keywords = {\n",
        "        \"adk_channels\":get_adk_channels_filter_from_obj,\n",
        "        \"bc_channel\":get_bc_channels_filter_from_obj\n",
        "    }\n",
        "    formatable_fields = get_formatable_fields(string)\n",
        "    resulted_metrics = {}\n",
        "    for f in formatable_fields:\n",
        "        if \".\" in f:\n",
        "            f = f.split(\".\")[0]\n",
        "            \n",
        "        if f in extra_parameters.keys():\n",
        "            resulted_metrics[f] = extra_parameters[f]\n",
        "        elif f in keywords:\n",
        "            resulted_metrics[f] = keywords[f](obj)\n",
        "        else:\n",
        "            resulted_metrics[f] = obj.get_parameter(f)\n",
        "            \n",
        "        if resulted_metrics[f] is None:\n",
        "            resulted_metrics[f] = \"\"\n",
        "\n",
        "    new_string = string.format(**resulted_metrics)\n",
        "\n",
        "    return(new_string)\n",
        "\n",
        "def get_delta_months(d1, d2):\n",
        "    return d2.month - d1.month + 12*(d2.year - d1.year)\n",
        "\n",
        "def get_version_to_use(pgm, edate):\n",
        "    pgm_sdate = strToDateTime(pgm.first_version_date)\n",
        "    delta_monts = get_delta_months(pgm_sdate, edate)\n",
        "    return(pgm.first_version_id + delta_monts)\n",
        "    \n",
        "def get_program_from_name(program_name):\n",
        "    programs_conf = [Program(**p) for p in get_s3_file(bucket=S3_BUCKET_CONF, keys=PROGRAM_CONF_KEY)]\n",
        "    program = get_object_from_object_list(\n",
        "        obj_list=programs_conf,\n",
        "        key=\"name\",\n",
        "        value=program_name \n",
        "    )\n",
        "    return(program[0])\n",
        "\n",
        "@timeit\n",
        "def load_sell_data(sdate, edate, program_id, version_id, is_pyspark=False, local=False):\n",
        "    bucket = S3_SELL_LOG\n",
        "    file_pattern = 's3a://adomik-pipe-output-prod/data-pipeline-outputs/tmp/Daily/UNF/{}/*/{}/{:02d}/{:02d}/market_watch_metrics/*.parquet'\n",
        "    prefix = \"{sell_prefix}{pgm_id}/{version_id}/\".format(\n",
        "        sell_prefix=S3_SELL_PREFIX,\n",
        "        pgm_id=program_id,\n",
        "        version_id=version_id\n",
        "    )\n",
        "        \n",
        "    date_list = build_dates_list(sdate, edate)\n",
        "    dir_list = build_dir_list(\n",
        "        bucket=bucket,\n",
        "        key_to_format=prefix+\"{dte.year}/{dte.month:02d}/{dte.day:02d}/market_watch_metrics\",\n",
        "        replace_params={\n",
        "            \"dte\":date_list\n",
        "        },\n",
        "        verify_if_exists=False\n",
        "    )\n",
        "    key_list = []\n",
        "    for d in dir_list:\n",
        "        key_list += list(get_matching_s3_keys(\n",
        "            bucket=bucket,\n",
        "            prefix=d,\n",
        "            suffix='.parquet'\n",
        "        ))\n",
        "    \n",
        "    if len(key_list) > 0:\n",
        "        if is_pyspark:\n",
        "            pyspark_key_list = [\"s3a://\"+bucket+\"/\"+d for d in key_list]\n",
        "\n",
        "            new_df =  sqlContext.read.format('parquet')\\\n",
        "            .option(\"header\",True)\\\n",
        "            .load(pyspark_key_list)\\\n",
        "            .withColumn('file_name', input_file_name())\\\n",
        "            .withColumn('year', split(c('file_name'), '/')[9])\\\n",
        "            .withColumn('month', split(c('file_name'), '/')[10])\\\n",
        "            .withColumn('day', split(c('file_name'), '/')[11])\n",
        "            new_df = new_df.toPandas()\n",
        "        else:\n",
        "            dfs_list = []\n",
        "            for ind, k in enumerate(key_list):\n",
        "              progressBar(ind+1, len(key_list), bar_length=100)\n",
        "              temp_df = get_s3_file(\n",
        "                  bucket=bucket,\n",
        "                  keys=k,\n",
        "                  add_concat_column=[\n",
        "\n",
        "                      {\n",
        "                          \"filename_ind\":6,\n",
        "                          \"new_col_name\":\"year\"\n",
        "                      },\n",
        "\n",
        "                      {\n",
        "                          \"filename_ind\":7,\n",
        "                          \"new_col_name\":\"month\"\n",
        "                      },\n",
        "                      {\n",
        "                          \"filename_ind\":8,\n",
        "                          \"new_col_name\":\"day\"\n",
        "                      }\n",
        "                  ],\n",
        "                  local=local\n",
        "              )\n",
        "              temp_df = temp_df\\\n",
        "              .groupby([\"month\", \"buyer_id\", \"brand_id\", \"sales_channel\"], as_index=False)\\\n",
        "              .agg({\n",
        "                  \"revenue_resold\":\"sum\",\n",
        "                  \"revenue_deal\":\"sum\",\n",
        "                  \"impressions_resold\":\"sum\"\n",
        "              })\n",
        "              dfs_list.append(temp_df)\n",
        "            new_df = pd.concat(dfs_list)\n",
        "            new_df = optimize_df(new_df)\n",
        "            %reset_selective -f \"^dfs_list$\"\n",
        "    return (new_df)\n",
        "\n",
        "  \n",
        "def get_sell_newsletter_logs(sdate, edate, program, local=False, is_pyspark=False, pattern=''):\n",
        "    version_id = get_version_to_use(\n",
        "        pgm=program,\n",
        "        edate=edate\n",
        "    )\n",
        "    sell_newsletter_logs = load_sell_data(\n",
        "        sdate=sdate,\n",
        "        edate=edate,\n",
        "        program_id=program.program_id,\n",
        "        version_id=version_id,\n",
        "        is_pyspark=is_pyspark,\n",
        "        local=local\n",
        "    )\n",
        "    if is_pyspark:\n",
        "        labelled_sell_newsletter_logs = label_sell_data_pyspark(sell_newsletter_logs)\n",
        "    else:\n",
        "        labelled_sell_newsletter_logs = label_sell_data(sell_newsletter_logs)\n",
        "    return(labelled_sell_newsletter_logs)\n",
        "    \n",
        "\n",
        "@timeit\n",
        "def get_gdx_logs(start_date, end_date, seller, file_name, local=False, is_pyspark=False, pattern='', join_bc=True):\n",
        "    path_root = \"s3a://\"\n",
        "    bucket = S3_GDX_LOG\n",
        "    list_of_dates = build_dates_list(sdate=start_date, edate=end_date)\n",
        "    \n",
        "    if not isinstance(seller, list):\n",
        "        seller = [seller]\n",
        "        \n",
        "    seller_id_list = [s.seller_id for s in seller]\n",
        "    key_to_format = \"{seller}/{date}/rtb/auctions/\"+file_name\n",
        "        \n",
        "    replace_params = {\n",
        "        \"seller\":seller_id_list,\n",
        "        \"date\":dateTimeToList(list_of_dates)\n",
        "    }\n",
        "\n",
        "    list_of_dir = build_dir_list(\n",
        "        bucket=bucket,\n",
        "        key_to_format=key_to_format,\n",
        "        replace_params=replace_params\n",
        "    )\n",
        "    if len(list_of_dir) > 0:\n",
        "        if is_pyspark:\n",
        "            pyspark_list_of_dir = [path_root+bucket+\"/\"+d for d in list_of_dir]\n",
        "            \n",
        "            new_df =  sqlContext.read.format('csv')\\\n",
        "            .option(\"sep\",\"\\t\")\\\n",
        "            .option(\"header\",True)\\\n",
        "            .load(pyspark_list_of_dir)\\\n",
        "            .withColumn('Date', date_from_filename_udf(input_file_name()))\n",
        "            new_df = new_df.toPandas()\n",
        "        else:\n",
        "            new_df = get_s3_file(\n",
        "                bucket=bucket,\n",
        "                keys=list_of_dir,\n",
        "                add_concat_column=[\n",
        "                    {\n",
        "                        \"filename_ind\": 1,\n",
        "                        \"new_col_name\": \"Date\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"filename_ind\": 0,\n",
        "                        \"new_col_name\": \"Seller_id\"\n",
        "                    }\n",
        "                ],\n",
        "                local=local\n",
        "            )\n",
        "            \n",
        "    return(new_df)\n",
        "\n",
        "def transform_to_compar(df, format_parameters):\n",
        "        cv_params = format_parameters[\"compared_value\"]\n",
        "        if cv_params[\"type\"] == \"is_any_of\":\n",
        "            metrics_to_select = cv_params[\"param\"]\n",
        "        elif cv_params[\"type\"] == \"contains\":\n",
        "            metrics_to_select = []\n",
        "            for x in df.columns:\n",
        "                if cv_params[\"param\"] in x[1]:\n",
        "                    metrics_to_select.append(x[1])\n",
        "\n",
        "        columns_to_select = metrics_to_select + [\"all\"]\n",
        "        new_df = df.fillna(0.)\n",
        "        root_values = list(set(new_df.columns.get_level_values(0)))\n",
        "        idx = pd.IndexSlice\n",
        "        for cl in root_values:\n",
        "            new_df[cl, \"all\"] = new_df[cl].sum(axis=1)\n",
        "        new_df = new_df.loc[:, idx[:, columns_to_select]]\n",
        "        if \"aggregate_comp\" in cv_params:\n",
        "            for cl in root_values:\n",
        "                new_df[cl, cv_params[\"aggregate_comp\"]] = new_df[cl].sum(metrics_to_select)[cv_params[\"aggregate_comp\"], \"all\"]\n",
        "            new_df = new_df.loc[:, idx[:, [cv_params[\"aggregate_comp\"], \"all\"]]]\n",
        "            \n",
        "        return(new_df)\n",
        "\n",
        "def aggregate_df(df, agg_params):\n",
        "    formated_df = df.copy()\n",
        "    if len(agg_params[\"keys\"]) > 0 or len(agg_params[\"groups\"]) > 0:\n",
        "        formated_df = pd.pivot_table(\n",
        "            data=formated_df,\n",
        "            index=agg_params[\"keys\"],\n",
        "            columns=agg_params[\"groups\"],\n",
        "            aggfunc=agg_params[\"values\"],\n",
        "        )\n",
        "        if isinstance(formated_df.index, pd.MultiIndex):\n",
        "          converted_ind_list = []\n",
        "          for ind in range(len(formated_df.index.levels)):\n",
        "            converted_ind_list.append(unoptimize_df(formated_df.index.get_level_values(ind)))\n",
        "          formated_df.index = converted_ind_list\n",
        "        else:\n",
        "          formated_df.index = unoptimize_df(formated_df.index)\n",
        "          \n",
        "        if isinstance(formated_df.columns, pd.MultiIndex):\n",
        "          converted_ind_list = []\n",
        "          for ind in range(len(formated_df.columns.levels)):\n",
        "            converted_ind_list.append(unoptimize_df(formated_df.columns.get_level_values(ind)))\n",
        "          formated_df.columns = converted_ind_list\n",
        "        else:\n",
        "          formated_df.columns = unoptimize_df(formated_df.columns)\n",
        "        if agg_params[\"add_margins\"]:\n",
        "            if isinstance(formated_df.columns, pd.MultiIndex):\n",
        "                for cl in list(set(formated_df.columns.get_level_values(0))):\n",
        "                    formated_df[cl, \"all\"] = formated_df[cl].sum(axis=1)\n",
        "            else:\n",
        "                formated_df[\"all\"] = formated_df.sum(axis=1)\n",
        "            formated_df.loc['all'] = formated_df.sum(axis=0)\n",
        "            \n",
        "        if len(agg_params[\"keys\"]) == 0:\n",
        "            formated_df = pd.DataFrame(formated_df.stack()).T\n",
        "    else:\n",
        "        formated_df = pd.DataFrame(formated_df.agg(agg_params[\"values\"])).T\n",
        "    return(formated_df)\n",
        "\n",
        "@timeit\n",
        "def lab_adk(df, dimension, drop_non_labelled=False, local=False):\n",
        "    adk_labelling_bucket = \"adomik-labeling\"\n",
        "    adk_labelling_key = \"type=adomik/platform_code=adk/mapping.csv\"\n",
        "\n",
        "    df_lab = get_s3_file(\n",
        "        bucket=adk_labelling_bucket,\n",
        "        keys=adk_labelling_key,\n",
        "        sep=\",\",\n",
        "        header=None,\n",
        "        local=local\n",
        "    )\n",
        "\n",
        "    df_lab = df_lab.rename(columns={0:dimension, 1:dimension+\"_labelled\"})\n",
        "    df_lab[dimension] = \"adk-\"+df_lab[dimension].astype(str)\n",
        "    df_tmp = df.merge(df_lab, on=dimension, how=\"left\")\n",
        "    df_tmp.loc[df_tmp[dimension+\"_labelled\"].isna(), dimension+\"_labelled\"] = df_tmp[dimension]\n",
        "    if drop_non_labelled:\n",
        "        df_tmp[dimension] = df_tmp[dimension+\"_labelled\"]\n",
        "        df_tmp = df_tmp.drop(dimension+\"_labelled\", axis=1)\n",
        "    \n",
        "    return df_tmp\n",
        "\n",
        "def lab_adk_pyspark(df, dimension, drop_non_labelled=False):\n",
        "    file_pattern = 's3a://adomik-labeling/type=adomik/platform_code=adk'\n",
        "    \n",
        "    df_lab = sqlContext\\\n",
        "                .read\\\n",
        "                .format('csv')\\\n",
        "                .option('header', False)\\\n",
        "                .option('basePath', 's3a://adomik-labeling/type=adomik/platform_code=adk')\\\n",
        "                .load(file_pattern)\\\n",
        "                .withColumnRenamed('_c0', dimension)\\\n",
        "                .withColumn(dimension, concat_ws('-', lit('adk'), dimension))\\\n",
        "                .withColumnRenamed('_c1', '{}_labelled'.format(dimension))\n",
        "    \n",
        "    df_tmp = df.join(broadcast(df_lab), dimension, how='leftouter')\n",
        "    if drop_non_labelled:\n",
        "        df_tmp = df_tmp.drop(dimension)\\\n",
        "        .withColumnRenamed('{}_labelled'.format(dimension), dimension)\n",
        "    \n",
        "    return df_tmp\n",
        "\n",
        "@timeit\n",
        "def lab_ext(df, dimension, drop_non_labelled=False, local=False):\n",
        "    print(\"df :\", str(sys.getsizeof(df)))\n",
        "    adk_labelling_bucket = \"adomik-labeling\"\n",
        "    mw_platforms = ['anx', 'gdx', 'rxb', 'idx', 'opx']\n",
        "    adk_labelling_key = \"type=demand/platform_code={}/dimension={}/mapping.csv\"\n",
        "    dir_list =  [adk_labelling_key.format(platform, dimension) for platform in mw_platforms]\n",
        "    df_lab = get_s3_file(\n",
        "        bucket=adk_labelling_bucket,\n",
        "        keys=dir_list,\n",
        "        sep=\",\",\n",
        "        header=None,\n",
        "        add_concat_column=[\n",
        "            {\n",
        "                \"filename_ind\":1,\n",
        "                \"new_col_name\":\"platform_code_to_format\"\n",
        "            },\n",
        "        ],\n",
        "        local=local\n",
        "        \n",
        "    )\n",
        "    print(\"df_lab_avt :\", str(sys.getsizeof(df_lab)))\n",
        "    df_lab[0] = df_lab[\"platform_code_to_format\"].str.split(\"=\").str[1].astype(str) + \"-\" + df_lab[0].astype(str)\n",
        "    df_lab = df_lab.drop(\"platform_code_to_format\", axis=1)\n",
        "    df_lab = df_lab.rename(columns={0:dimension, 1:dimension+\"_labelled\"})\n",
        "    print(\"df_lab_aps :\", str(sys.getsizeof(df_lab)))\n",
        "    df_tmp = df.merge(df_lab, on=dimension, how=\"left\")\n",
        "    print(\"df_tmp_avt :\", str(sys.getsizeof(df_tmp)))\n",
        "    df_tmp.loc[df_tmp[dimension+\"_labelled\"].isna(), dimension+\"_labelled\"] = df_tmp[dimension]\n",
        "    if drop_non_labelled:\n",
        "        df_tmp[dimension] = df_tmp[dimension+\"_labelled\"]\n",
        "        df_tmp = df_tmp.drop(dimension+\"_labelled\", axis=1)\n",
        "    print(\"df_tmp_aps :\", str(sys.getsizeof(df_tmp)))\n",
        "    \n",
        "    return df_tmp\n",
        "\n",
        "def lab_ext_pyspark(df, dimension, drop_non_labelled=False):\n",
        "    \n",
        "    file_pattern = 's3a://adomik-labeling/type=demand/platform_code={}/dimension={}/'\n",
        "    mw_platforms = ['anx', 'gdx', 'rxb', 'idx', 'opx']\n",
        "    dir_list =  [file_pattern.format(platform, dimension) for platform in mw_platforms]\n",
        "    \n",
        "    df_lab = sqlContext\\\n",
        "                .read\\\n",
        "                .format('csv')\\\n",
        "                .option('header', False)\\\n",
        "                .option('basePath', 's3a://adomik-labeling/type=demand/')\\\n",
        "                .load(dir_list)\\\n",
        "                .withColumnRenamed('_c0', dimension)\\\n",
        "                .withColumn(dimension, concat_ws('-', 'platform_code', dimension))\\\n",
        "                .withColumnRenamed('_c1', '{}(mw_label)'.format(dimension))\\\n",
        "                .select(dimension, '{}_labelled'.format(dimension))\n",
        "                \n",
        "    df_tmp = df.join(broadcast(df_lab), dimension, how='leftouter')\n",
        "    if drop_non_labelled:\n",
        "        df_tmp = df_tmp.drop(dimension)\\\n",
        "        .withColumnRenamed('{}_labelled'.format(dimension), dimension)\n",
        "    \n",
        "    return df_tmp\n",
        "\n",
        "@timeit\n",
        "def label_sell_data(df, dropna=True, local=False):\n",
        "    df_tmp = df.copy()\n",
        "    dimensions = ['buyer_id', 'brand_id', 'sales_channel']\n",
        "    for dimension in dimensions:\n",
        "        df_tmp = pd.concat(\n",
        "            [\n",
        "              lab_adk(df_tmp[df_tmp[dimension].str.startswith('adk')], dimension, drop_non_labelled=True, local=local),\n",
        "              lab_ext(df_tmp[~df_tmp[dimension].str.startswith('adk')], dimension, drop_non_labelled=True, local=local)\n",
        "            ],\n",
        "            sort=True\n",
        "        )\n",
        "        df_tmp = optimize_df(df_tmp)\n",
        "    if dropna:\n",
        "        df_tmp = df_tmp.dropna(subset=dimensions)\n",
        "    \n",
        "    return(df_tmp)\n",
        "\n",
        "def label_sell_data_pyspark(df, dropna=True):\n",
        "    df_tmp = df\n",
        "    dimensions = ['buyer_id', 'brand_id', 'device_type', 'format', 'sales_channel']\n",
        "    for dimension in dimensions:\n",
        "        df_adk = lab_adk(df_tmp.filter(c(dimension).startswith('adk')), dimension)\n",
        "        df_ext = lab_ext(df_tmp.filter(~c(dimension).startswith('adk')), dimension)\n",
        "        df_tmp = df_adk.union(df_ext)\n",
        "    if dropna:\n",
        "        df_tmp = df_tmp.dropna(subset=dimensions)\n",
        "    return df_tmp\n",
        "\n",
        "def get_pardot_template(template_id):\n",
        "    p = PardotAPI(\n",
        "        email=PARDOT_EMAIL,\n",
        "        password=PARDOT_PASSWD,\n",
        "        user_key=PARDOT_API_KEY\n",
        "    )\n",
        "    if p.authenticate():\n",
        "      pdt_template = p.emailtemplates.read(emailTemplateID=template_id)['emailTemplate'][\"htmlMessage\"]\n",
        "      pdt_template = pdt_template.replace('\\n', '').replace('\\t','')\n",
        "      return(pdt_template)\n",
        "    else:\n",
        "      print(\"Authentication problem, please try again later.\")\n",
        "      return(\"\")\n",
        "    \n",
        "def get_css_stylesheet(template_html):\n",
        "  css_stylesheet = re.findall(r'<!-- css_stylesheet -->(.*?)<!-- /css_stylesheet -->',template_html)\n",
        "  return(css_stylesheet)\n",
        "    \n",
        "def get_elements_from_pardot_template(template_html):\n",
        "    elements_template = dict(re.findall(r'<!-- element_template:(.*?) -->(.*?)<!-- /element_template:.*? -->',template_html))\n",
        "    return(elements_template)\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMm2csraYPD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N38_D15-wGSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJNvSRV-j90U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Snapshot():\n",
        "    \"\"\"\n",
        "    The Snapshot represents the report in the largest term\n",
        "    \n",
        "    Args:\n",
        "        parameters (dict): Dict containing all the parameters\n",
        "\n",
        "    Attributs:\n",
        "        parameters (dict{\n",
        "            start_date (string): The start date of the snapshot (YYYY-MM-YY) (Default value = yesterday)\n",
        "            end_sate (SnapshotData): The end date of the snapshot (YYYY-MM-YY) (Default value = today)\n",
        "            seller (Seller): The Seller affected to the report\n",
        "            renamer (dict): The dict containing all the labelling of columns\n",
        "            snapshot_type (string): The type of snapshot you want to create\n",
        "        })\n",
        "        elements (list[Element]): A list of all the elements contained in the Snapshot\n",
        "        email (Email): The email related to the Snapshot\n",
        "    \"\"\"\n",
        "    def __init__(self, parameters):\n",
        "        columns_conf = get_s3_file(S3_BUCKET_CONF, COLUMN_CONF_KEY)\n",
        "        \n",
        "        default_parameters = {\n",
        "            \"columns_conf\":columns_conf,\n",
        "            \"snapshot_type\":\"mandatory\"\n",
        "        }\n",
        "        \n",
        "        self.parameters = check_parameters(parameters, default_parameters)\n",
        "        self.elements = []\n",
        "        self.cache = []\n",
        "        \n",
        "    def build_snapshot_data(self, sd_params):\n",
        "        \"\"\"\n",
        "        Build a SnapshotData from its parameters\n",
        "\n",
        "        Args:\n",
        "            sd_params (dict): The parameters of the SnapshotData (See SnapshotData for mor informations)\n",
        "\n",
        "        Returns:\n",
        "            (SnapshotData) The built SnapshotData\n",
        "        \"\"\"\n",
        "        return(SnapshotData(self, sd_params))\n",
        "    \n",
        "    def add_element(self, el):\n",
        "        \"\"\"\n",
        "        Add an element to the Snapshot\n",
        "\n",
        "        Args:\n",
        "            el (Element): The element to add\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        if isinstance(el, list):\n",
        "            self.elements += el\n",
        "        else:\n",
        "            self.elements.append(el)\n",
        "    \n",
        "    def build_email(self, email_params):\n",
        "        \"\"\"\n",
        "        Build the email related to the Snapshot\n",
        "\n",
        "        Args:\n",
        "            email_params (dict): The parameters of the Email (See Email for mor informations)\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        new_email = Email(self, parameters=email_params) \n",
        "        self.email = new_email\n",
        "        \n",
        "    def send(self):\n",
        "        \"\"\"\n",
        "        Send the email\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.email.send_email()\n",
        "        \n",
        "    def get_parameter(self, param):\n",
        "        if param in self.parameters:\n",
        "            return(self.parameters[param])\n",
        "        else:\n",
        "            print(\"The parameters {param} does not exist in Snapshot object.\".format(param=param))\n",
        "            return(None)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW9FWWFpj90W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AggregatorEngine:\n",
        "    def __init__(self, snapshot, parameters):\n",
        "        self.snapshot = snapshot\n",
        "        default_parameters = {\n",
        "            \"keys\": [],\n",
        "            \"values\": {},\n",
        "            \"groups\": [],\n",
        "            \"sort\": {},\n",
        "            \"pre_agg_filter\": \"\",\n",
        "            \"post_agg_filter\": \"\",\n",
        "            \"select\": [],\n",
        "            \"limit\":-1,\n",
        "            \"compared_value\":{},\n",
        "            \"custom_fields_conf\":{},\n",
        "            \"add_margins\":False\n",
        "        }\n",
        "        self.parameters = check_parameters(parameters, default_parameters)\n",
        "        \n",
        "    def format_filters(self, obj):\n",
        "        for prefix in [\"pre\", \"post\"]:\n",
        "            self.parameters[prefix+\"_agg_filter\"] = format_to_universal_metrics(\n",
        "                obj=obj,\n",
        "                string=self.get_parameter(prefix+\"_agg_filter\")\n",
        "            )\n",
        "            \n",
        "    def get_values_type(self, df=pd.DataFrame()):\n",
        "        values_type = {\n",
        "            \"calculated\":{},\n",
        "            \"native\":{},\n",
        "            \"custom\":{\n",
        "                \"evolution\":{},\n",
        "                \"share_of_voice\":{}\n",
        "            },\n",
        "            \"bc\":{}\n",
        "        }\n",
        "        calc_values_conf = self.snapshot.get_parameter(\"columns_conf\")[\"calculated\"]\n",
        "        for v in self.get_parameter(\"values\"):\n",
        "            if v in df.columns:\n",
        "                values_type[\"native\"][v] = self.get_parameter(\"values\")[v]\n",
        "            else:\n",
        "                native_name = v\n",
        "                for c_field in values_type[\"custom\"]:\n",
        "                    if v.endswith(\"_\"+c_field):\n",
        "                        native_name = v.replace(\"_\"+c_field, \"\")\n",
        "                        values_type[\"custom\"][c_field][native_name] = self.get_parameter(\"values\")[v]\n",
        "                        \n",
        "                if native_name in calc_values_conf:\n",
        "                    req_fields = calc_values_conf[native_name][\"required_fields\"]\n",
        "                    values_type[\"calculated\"][native_name] = self.get_parameter(\"values\")[v]\n",
        "                    for k in req_fields:\n",
        "                        if k.endswith(\"_bc\"):\n",
        "                            new_bc_name = k.replace(\"_bc\", \"\")\n",
        "                            values_type[\"bc\"][new_bc_name] = req_fields[k]\n",
        "                        else:\n",
        "                            values_type[\"native\"][k] = req_fields[k]\n",
        "                else:\n",
        "                    values_type[\"native\"][native_name] = self.get_parameter(\"values\")[v]\n",
        "        return(values_type)\n",
        "    \n",
        "    def format_snapshot_data_df(self, df, snapshot_data):\n",
        "        new_df = df.copy()\n",
        "        self.values_type = self.get_values_type(df=new_df)\n",
        "        self.format_filters(snapshot_data)\n",
        "        new_df = self.filter_df(\n",
        "            df=new_df,\n",
        "            filter_str=self.get_parameter(\"pre_agg_filter\", default=\"\")\n",
        "        )\n",
        "        new_df = self.aggregate(df=new_df)\n",
        "        if len(self.values_type[\"bc\"]) > 0:\n",
        "            new_df = self.join_bc_data(\n",
        "                df=new_df,\n",
        "                snapshot_data=snapshot_data\n",
        "            )\n",
        "        \n",
        "        if len(self.values_type[\"calculated\"]) > 0:\n",
        "            new_df = self.add_calculated_metrics(\n",
        "                df=new_df,\n",
        "                snapshot_data=snapshot_data\n",
        "            )\n",
        "        new_df = self.set_compar(df=new_df)\n",
        "        new_df = self.set_custom_fields(df=new_df)\n",
        "        new_df = self.filter_df(\n",
        "            df=new_df,\n",
        "            filter_str=self.get_parameter(\"post_agg_filter\", default=\"\")\n",
        "        )\n",
        "        new_df = self.select_only_values(df=new_df)\n",
        "        new_df = self.select(df=new_df)\n",
        "        new_df = self.sort(df=new_df)\n",
        "        new_df = self.limit(df=new_df)\n",
        "        return(new_df)\n",
        "    \n",
        "    def select_only_values(self, df):\n",
        "        if len(self.get_parameter(\"values\")) > 0 and len(self.get_parameter(\"select\")) == 0:\n",
        "            new_df = df[self.get_parameter(\"values\").keys()]\n",
        "        else:\n",
        "            new_df = df.copy()\n",
        "        return(new_df)\n",
        "    \n",
        "    def set_custom_fields(self, df):\n",
        "        new_df = cp.deepcopy(df)\n",
        "        for _field, _values in self.values_type[\"custom\"].items():\n",
        "            if len(_values) > 0:\n",
        "                if _field == \"share_of_voice\":\n",
        "                    new_df = self.add_share_of_voice_field(df=new_df)\n",
        "                elif _field == \"evolution\":\n",
        "                    new_df = self.add_evolution_field(df=new_df)\n",
        "        return(new_df)\n",
        "    \n",
        "    def add_share_of_voice_field(self, df):\n",
        "        default_confs = {\n",
        "            \"axis\":1,\n",
        "            \"compared_to\":\"self\"\n",
        "        }\n",
        "        \n",
        "        new_df = df.copy()\n",
        "        root_values = list(set(new_df.columns.get_level_values(0)))\n",
        "        for cl in self.values_type[\"custom\"][\"share_of_voice\"]:\n",
        "            cl_cov_conf = check_parameters(self.get_parameter(\"custom_fields_conf\").get(\"share_of_voice\", {cl:{}})[cl], default_confs)\n",
        "            if cl_cov_conf[\"compared_to\"] == \"self\":\n",
        "              compared_to_col = cl\n",
        "            else:\n",
        "              compared_to_col = cl_cov_conf[\"compared_to\"]\n",
        "            if isinstance(new_df.columns, pd.MultiIndex):\n",
        "                sum_metric = new_df.drop(\"all\", errors='ignore')\\\n",
        "                .drop(\"all\", errors='ignore', axis=1, level=1)[compared_to_col]\\\n",
        "                .sum(axis=cl_cov_conf[\"axis\"])\n",
        "                \n",
        "                if cl_cov_conf[\"axis\"] == 1:\n",
        "                    for sel_col in new_df[cl]:\n",
        "                        new_df[cl+\"_share_of_voice\", sel_col] = new_df[cl, sel_col]/sum_metric\n",
        "                else:\n",
        "                    divided_df = new_df.loc[:,cl].div(sum_metric)\n",
        "                    divided_df.columns = [[cl+\"_share_of_voice\"] * len(divided_df.columns), divided_df.columns]\n",
        "                    new_df = new_df.join(divided_df)\n",
        "            else:\n",
        "                sum_metric = new_df.drop(\"all\", errors='ignore')\\\n",
        "                .drop(\"all\", errors='ignore', axis=1)[compared_to_col]\\\n",
        "                .sum(axis=cl_cov_conf[\"axis\"])\n",
        "                if cl_cov_conf[\"axis\"] == 1:\n",
        "                    new_df[cl+\"_share_of_voice\"] = new_df[cl]/sum_metric\n",
        "                else:\n",
        "                    new_df.loc[:,cl+\"_share_of_voice\"] = new_df.loc[:,cl].div(sum_metric)\n",
        "            \n",
        "        return(new_df)\n",
        "    \n",
        "    def add_evolution_field(self, df):\n",
        "        default_confs = {\n",
        "            \"axis\":1,\n",
        "            \"compared_to\":\"mandatory\"\n",
        "        }\n",
        "        new_df = df.copy()\n",
        "        root_values = list(set(new_df.columns.get_level_values(0)))\n",
        "        for cl in self.values_type[\"custom\"][\"evolution\"]:\n",
        "            cl_cov_conf = check_parameters(self.get_parameter(\"custom_fields_conf\").get(\"evolution\", {cl:{}})[cl], default_confs)\n",
        "            sum_metric = new_df[cl].sum(axis=cl_cov_conf[\"axis\"])\n",
        "            if isinstance(new_df.columns, pd.MultiIndex):\n",
        "                for sel_col in new_df[cl]:\n",
        "                    if not sel_col == cl_cov_conf[\"compared_to\"]:\n",
        "                        new_df[cl+\"_evolution\", sel_col] = (new_df[cl, sel_col]-new_df[cl, cl_cov_conf[\"compared_to\"]])/new_df[cl, cl_cov_conf[\"compared_to\"]]\n",
        "            else:\n",
        "                print(\"The evolution custom field is not available for simple index.\")\n",
        "                \n",
        "            new_df[cl+\"_evolution\", cl_cov_conf[\"compared_to\"]] = 0\n",
        "        return(new_df)\n",
        "    \n",
        "    @timeit\n",
        "    def set_compar(self, df):\n",
        "        agg_params = cp.deepcopy(self.parameters)\n",
        "        if len(self.get_parameter(\"compared_value\")) > 0:\n",
        "            formated_df = transform_to_compar(df, agg_params)\n",
        "        else:\n",
        "            formated_df = df\n",
        "        return(formated_df)\n",
        "        \n",
        "    @timeit\n",
        "    def join_bc_data(self, df, snapshot_data):\n",
        "        join_keys = cp.deepcopy(self.get_parameter(\"keys\"))\n",
        "        bc_values = self.values_type[\"bc\"]\n",
        "        if \"channel\" in join_keys:\n",
        "            join_keys.drop(\"channel\")\n",
        "        \n",
        "        filter_str = format_to_universal_metrics(\n",
        "            obj=snapshot_data,\n",
        "            string='channel in ({bc_channel})'\n",
        "        )\n",
        "        \n",
        "        sd_params = {\n",
        "            \"pattern\":\"po_adx\",\n",
        "            \"file_name\":\"floor_rule_metrics.csv\"\n",
        "        }\n",
        "        \n",
        "        bc_sd = self.snapshot.build_snapshot_data(\n",
        "            sd_params=sd_params\n",
        "        )\n",
        "        \n",
        "        bc_perf_parameters = {\n",
        "            \"keys\": join_keys,\n",
        "            \"values\": bc_values,\n",
        "            \"pre_agg_filter\": filter_str\n",
        "        }\n",
        "        \n",
        "        bc_agg_engine = AggregatorEngine(\n",
        "            snapshot=snapshot_data.snapshot,\n",
        "            parameters=bc_perf_parameters\n",
        "        )\n",
        "\n",
        "        bc_perfs = bc_agg_engine\\\n",
        "        .format_snapshot_data_df(\n",
        "            df=bc_sd.dataframe,\n",
        "            snapshot_data=bc_sd\n",
        "        )\\\n",
        "        .rename(columns={k:k+\"_bc\" for k in bc_values})\n",
        "        \n",
        "        if len(join_keys) > 0:\n",
        "            joined_df = pd.merge(df, bc_perfs, on=join_keys)\n",
        "        else:\n",
        "            joined_df = pd.concat([df, bc_perfs], axis=1, join=\"inner\")\n",
        "    \n",
        "        return(joined_df)\n",
        "    \n",
        "    @timeit\n",
        "    def aggregate(self, df):\n",
        "        agg_params = self.get_parameters_with_value_type(\"native\")\n",
        "        \n",
        "        if len(agg_params[\"values\"]) > 0:\n",
        "            aggregated_df = aggregate_df(\n",
        "                df=df,\n",
        "                agg_params=agg_params\n",
        "            )\n",
        "        else:\n",
        "            aggregated_df = df.copy()\n",
        "        \n",
        "        return (aggregated_df)\n",
        "    \n",
        "    @timeit\n",
        "    def add_calculated_metrics(self, df, snapshot_data):\n",
        "        calculated_df = df.copy()\n",
        "        root_values = list(set(calculated_df.columns.get_level_values(0)))\n",
        "        calc_metrics = self.snapshot.get_parameter(\"columns_conf\")[\"calculated\"]\n",
        "        for cm, v in calc_metrics.items():\n",
        "            if cm in self.values_type[\"calculated\"]:\n",
        "                if is_l1_in_l2(root_values, list(v[\"required_fields\"])):\n",
        "                    if isinstance(calculated_df.columns, pd.MultiIndex):\n",
        "                        for sub_col in list(set(calculated_df.columns.get_level_values(1))):\n",
        "                            calculated_df[cm, sub_col] = calculated_df.xs(key=sub_col, level=1, axis=1).eval(v[\"calc\"])\n",
        "                    else:\n",
        "                        calculated_df[cm] = calculated_df.eval(v[\"calc\"])\n",
        "                else:\n",
        "                    print(\"Missing fields in dataframe. Required are : {req_fields}\".format(req_fields=str(v[\"required_fields\"])))\n",
        "        \n",
        "        return(calculated_df)\n",
        "    \n",
        "    @timeit\n",
        "    def select(self, df):\n",
        "        if len(self.get_parameter(\"select\")) > 0:\n",
        "            return(df[self.get_parameter(\"select\")])\n",
        "        else:\n",
        "            return(df)\n",
        "    \n",
        "    @timeit\n",
        "    def sort(self, df):\n",
        "        if len(self.get_parameter(\"sort\")) > 0:\n",
        "            return(df.sort_values(**self.get_parameter(\"sort\")))\n",
        "        else:\n",
        "            return(df)\n",
        "    \n",
        "    @timeit\n",
        "    def limit(self, df):\n",
        "        if self.get_parameter(\"limit\") >= 0:\n",
        "            return(df[:int(self.get_parameter(\"limit\"))])\n",
        "        else:\n",
        "            return(df)\n",
        "    \n",
        "    @timeit\n",
        "    def filter_df(self, df, filter_str):\n",
        "        if len(filter_str) > 0:\n",
        "            return(df.query(filter_str))\n",
        "        else:\n",
        "            return(df)\n",
        "        \n",
        "    def get_parameters_with_value_type(self, value_type):\n",
        "        new_params = cp.deepcopy(self.parameters)\n",
        "        new_params[\"values\"] = self.values_type[value_type]\n",
        "        return(new_params)\n",
        "    \n",
        "    def get_parameters_with_custom_value_type(self, value_type):\n",
        "        new_params = cp.deepcopy(self.parameters)\n",
        "        new_params[\"values\"] = self.values_type[\"custom\"][value_type]\n",
        "        return(new_params)\n",
        "    \n",
        "    @timeit\n",
        "    def get_parameter(self, param, default=None):\n",
        "        if param in self.snapshot.parameters:\n",
        "            return(self.snapshot.parameters[param])\n",
        "        elif param in self.parameters:\n",
        "            return(self.parameters[param])\n",
        "        else:\n",
        "            return(default)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqK5LIlvj90Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SnapshotData():\n",
        "    def __init__(self, snapshot, parameters):\n",
        "        self.snapshot = snapshot\n",
        "        default_parameters = {\n",
        "            \"recalculate_revenue\": True,\n",
        "            \"dataframe\": None,\n",
        "            \"file_name\": \"\",\n",
        "            \"pattern\": \"\",\n",
        "            \"pre_agg\": {}\n",
        "        }\n",
        "        self.parameters = check_parameters(parameters, default_parameters)\n",
        "        if self.get_parameter(\"dataframe\") is None:\n",
        "            self.dataframe = self.get_raw_data()\n",
        "        else:\n",
        "            print(\"debug\", \"ok1\")\n",
        "            self.dataframe = self.get_parameter(\"dataframe\")\n",
        "            self.dataframe = self.rename_columns(self.dataframe)\n",
        "        if len(self.get_parameter(\"pre_agg\")) > 0:\n",
        "            agg_engine = AggregatorEngine(\n",
        "                snapshot=self.snapshot,\n",
        "                parameters=self.get_parameter(\"pre_agg\")\n",
        "            )\n",
        "            self.aggregate(agg_engine=agg_engine, inplace=True)\n",
        "        print(\"debug\", \"ok2\")\n",
        "        self.update_columns()\n",
        "        \n",
        "    @timeit\n",
        "    def rename_columns(self, df):\n",
        "        name_mapping = {}\n",
        "        type_mapping = {}\n",
        "        rd = self.get_parameter(\"columns_conf\")[\"native\"]\n",
        "        \n",
        "        if len(self.get_parameter(\"currency\", default=\"\")) == 0:\n",
        "            self.parameters[\"currency\"] = self.guess_currency(df)\n",
        "        i = 0\n",
        "        for col_name, conf in rd.items():\n",
        "            progressBar(i+1, len(rd), bar_length=100)\n",
        "            for e_name in conf[\"ext_name\"]:\n",
        "                compared_name = e_name\n",
        "                if conf[\"type\"] == \"amount\":\n",
        "                     compared_name += \" ({cur})\".format(cur=self.get_parameter(\"currency\"))\n",
        "            \n",
        "                if compared_name in df.columns:\n",
        "                    present_col = compared_name\n",
        "                    break;\n",
        "                elif e_name in df.columns:\n",
        "                    present_col = e_name\n",
        "                    break;\n",
        "                else:\n",
        "                    present_col = \"\"\n",
        "                \n",
        "            if len(present_col) > 0:\n",
        "                name_mapping[present_col] = col_name\n",
        "                type_mapping[present_col] = conf[\"type\"]\n",
        "            i += 1\n",
        "                \n",
        "        df = df.rename(columns=name_mapping)\n",
        "        return(df)\n",
        "\n",
        "    def update_columns(self):\n",
        "        self.columns = {}\n",
        "        rd = self.get_parameter(\"columns_conf\")\n",
        "        all_columns = {j:rd[k][j] for k in rd for j in rd[k]}\n",
        "            \n",
        "        cols_list = list(set(self.dataframe.columns.get_level_values(0)))\n",
        "        if not list(self.dataframe.index.names) == [None]:\n",
        "            cols_list += list(self.dataframe.index.names)\n",
        "        for col in cols_list:\n",
        "            if col in all_columns:\n",
        "                col_name = col\n",
        "                pretty_name = all_columns[col][\"pretty_name\"]\n",
        "                col_type = all_columns[col][\"type\"]\n",
        "            elif col.split(\"(\")[-1].split(\")\")[0] in all_columns:\n",
        "                agg_col = col.split(\"(\")[-1].split(\")\")[0]\n",
        "                \n",
        "                col_name = agg_col\n",
        "                pretty_name = \"Total \" + all_columns[agg_col][\"pretty_name\"]\n",
        "                col_type = all_columns[agg_col][\"type\"]\n",
        "                \n",
        "                self.rename_column(col, agg_col)\n",
        "            elif col.endswith(\"_share_of_voice\"):\n",
        "                if col.replace(\"_share_of_voice\", \"\") in all_columns:\n",
        "                    col_name = col\n",
        "                    pretty_name = all_columns[col.replace(\"_share_of_voice\", \"\")][\"pretty_name\"]+\" share of voice\"\n",
        "                    col_type = \"perc\"\n",
        "                else:\n",
        "                    col_name = col\n",
        "                    pretty_name = col.replace(\"_share_of_voice\", \"\")+\" share of voice\"\n",
        "                    col_type = \"perc\"\n",
        "            elif col.endswith(\"_evolution\"):\n",
        "                if col.replace(\"_evolution\", \"\") in all_columns:\n",
        "                    col_name = col\n",
        "                    pretty_name = all_columns[col.replace(\"_evolution\", \"\")][\"pretty_name\"]+\" evolution\"\n",
        "                    col_type = \"perc\"\n",
        "                else:\n",
        "                    col_name = col\n",
        "                    pretty_name = col.replace(\"_evolution\", \"\")+\" evolution\"\n",
        "                    col_type = \"perc\"\n",
        "            elif col.endswith(\"_bc\"):\n",
        "                if col.replace(\"_bc\", \"\") in all_columns:\n",
        "                    col_name = col\n",
        "                    pretty_name = all_columns[col.replace(\"_bc\", \"\")][ \"pretty_name\"]+\" benchmark\"\n",
        "                    col_type = all_columns[col.replace(\"_bc\", \"\")][ \"type\"]\n",
        "                else:\n",
        "                    col_name = col\n",
        "                    pretty_name = col\n",
        "                    col_type = \"float\"\n",
        "            else:\n",
        "                col_name=col\n",
        "                pretty_name=col\n",
        "                col_type=\"string\"\n",
        "            \n",
        "            self.add_column(\n",
        "                col_name=col_name,\n",
        "                pretty_name=pretty_name,\n",
        "                col_type=col_type\n",
        "            )\n",
        "        #self.dataframe = self.dataframe.reset_index(drop=True)\n",
        "    \n",
        "    def aggregate(self, agg_engine=None, inplace=False):\n",
        "        \n",
        "        new_df = agg_engine.format_snapshot_data_df(\n",
        "            df=self.dataframe,\n",
        "            snapshot_data=self\n",
        "        )\n",
        "        \n",
        "        if inplace:\n",
        "            self.dataframe = new_df\n",
        "        else:\n",
        "            new_sd_params = cp.deepcopy(self.parameters)\n",
        "            new_sd_params[\"dataframe\"] = new_df\n",
        "            del new_sd_params[\"pre_agg\"]\n",
        "            new_sd = self.snapshot.build_snapshot_data(sd_params=new_sd_params)\n",
        "            return(new_sd)\n",
        "    \n",
        "    def get_raw_data(self, rename_columns=True, recalc_revenue=True):\n",
        "        params_to_use = {}\n",
        "        missing_args = []\n",
        "        \n",
        "        if self.get_parameter(\"pattern\") == \"po_adx\":\n",
        "            func = get_gdx_logs\n",
        "        elif self.get_parameter(\"pattern\") == \"sell_newsletter\":\n",
        "            func = get_sell_newsletter_logs\n",
        "        else:\n",
        "            print(\"Wrong pattern (or no pattern) SnapshotData.\")\n",
        "            return(pd.DataFrame())\n",
        "            \n",
        "        mandatory_args, optionnal_args = get_fct_args(func)\n",
        "        for p in mandatory_args+optionnal_args:\n",
        "            new_arg = self.get_parameter(p)\n",
        "            if new_arg is None and p in mandatory_args:\n",
        "                missing_args.append(p)\n",
        "            else:\n",
        "                params_to_use[p] = new_arg\n",
        "        if len(missing_args) > 0:\n",
        "            print(\"{missing_args} missing for pattern {pattern}.\".format(\n",
        "                missing_args=missing_args.join(\", \"),\n",
        "                pattern=self.get_parameter(\"pattern\")\n",
        "            ))\n",
        "            return(pd.DataFrame())\n",
        "        \n",
        "        scopes_list = [c['scope'] for c in self.snapshot.cache]\n",
        "        if params_to_use in scopes_list:\n",
        "            gross_df = self.snapshot.cache[scopes_list.index(params_to_use)]['data']\n",
        "        else:\n",
        "            #gross_df = df_ex_debug_algo\n",
        "            gross_df = func(**params_to_use)\n",
        "            self.snapshot.cache.append({\n",
        "                'scope':params_to_use,\n",
        "                'data':gross_df\n",
        "            })\n",
        "            \n",
        "        if rename_columns:\n",
        "            gross_df = self.rename_columns(gross_df)\n",
        "            \n",
        "        if recalc_revenue:\n",
        "            gross_df = recalculate_revenue(gross_df)\n",
        "        return(gross_df)\n",
        "\n",
        "    def guess_currency(self, df):\n",
        "        cols = list(set(df.columns.get_level_values(0)))\n",
        "        rd = self.get_parameter(\"columns_conf\")\n",
        "        all_columns = {j:rd[k][j] for k in rd for j in rd[k]}\n",
        "        col_mapping = {\n",
        "            all_columns[n][\"pretty_name\"]: {\n",
        "                \"name\": n,\n",
        "                \"type\": all_columns[n][\"type\"]\n",
        "            } for n in all_columns\n",
        "        }\n",
        "        for c in cols:\n",
        "            splited_name = c.split(\" (\")\n",
        "            if splited_name[0] in col_mapping:\n",
        "                if col_mapping[splited_name[0]][\"type\"] == \"amount\":\n",
        "                    return(splited_name[1].split(\")\")[0])\n",
        "        return(\"\")\n",
        "\n",
        "    def add_column(self, col_name, pretty_name=\"\", col_type=\"string\"):\n",
        "        new_col = Column(col_name, pretty_name=pretty_name, col_type=col_type)\n",
        "        self.columns[col_name] = new_col\n",
        "\n",
        "    def get_graphable_df(self, pretty_named=False, set_ind={}, columns_renamer={}):\n",
        "        new_df = self.dataframe\n",
        "        index_new_names = []\n",
        "        pretty_renamer = {}\n",
        "        if pretty_named:\n",
        "            for col in self.columns:\n",
        "                if col in columns_renamer:\n",
        "                    self.columns[col].pretty_name = columns_renamer[col]\n",
        "                new_col_name = \"\"\n",
        "                if self.columns[col].col_type == \"amount\":\n",
        "                    new_col_name += self.columns[col].pretty_name + \" ({cur})\".format(cur=self.get_parameter(\"currency\"))\n",
        "                elif self.columns[col].col_type == \"perc\":\n",
        "                    new_col_name += self.columns[col].pretty_name + \" (%)\"\n",
        "                else:\n",
        "                    new_col_name += self.columns[col].pretty_name\n",
        "                \n",
        "                if col in self.dataframe:\n",
        "                    pretty_renamer[col] = new_col_name\n",
        "                elif col in list(self.dataframe.index.names):\n",
        "                    index_new_names.append(new_col_name)\n",
        "\n",
        "        new_df = new_df.rename(columns=pretty_renamer)\n",
        "        if len(index_new_names) > 0:\n",
        "            new_df.index.names = index_new_names\n",
        "        return(new_df)\n",
        "\n",
        "    def rename_metrics(self, df, metrics, pretty=False, suffix=None):\n",
        "        if not metrics is None:\n",
        "            for m in metrics:\n",
        "                if pretty:\n",
        "                    new_name = metrics[m] + \" of \" + self.columns[m].pretty_name\n",
        "                else:\n",
        "                    new_name = m\n",
        "                if suffix is not None:\n",
        "                    new_name += str(suffix)\n",
        "                df = df.rename(columns={metrics[m] + \"(\" + m + \")\": new_name})\n",
        "        return(df)\n",
        "\n",
        "    def create_element(self, el_params):\n",
        "        if el_params[\"type\"] == \"graph\":\n",
        "            new_el = Graph(self, el_params)\n",
        "        elif el_params[\"type\"] == \"table\":\n",
        "            new_el = Table(self, el_params)\n",
        "        elif el_params[\"type\"] == \"text\":\n",
        "            new_el = Text(self, el_params)\n",
        "        return(new_el)\n",
        "\n",
        "    def show(self):\n",
        "        print(self.dataframe)\n",
        "\n",
        "    def rename_column(self, name_from, name_to):\n",
        "        self.columns[name_to] = self.columns[name_from]\n",
        "        self.columns[name_to].name = name_to\n",
        "        del self.columns[name_from]\n",
        "        self.dataframe = self.dataframe.rename(columns={name_from:name_to})\n",
        "        \n",
        "    def rename(self, renamer):\n",
        "        for former_name in renamer:\n",
        "            self.columns[renamer[former_name]] = self.columns[former_name]\n",
        "            del self.columns[former_name]\n",
        "            self.columns[renamer[former_name]].name = renamer[former_name]\n",
        "        self.dataframe.rename(renamer)\n",
        "\n",
        "    def get_parameter(self, param, default=None):\n",
        "        if param in self.snapshot.parameters:\n",
        "            return(self.snapshot.parameters[param])\n",
        "        elif param in self.parameters:\n",
        "            return(self.parameters[param])\n",
        "        else:\n",
        "            return(default)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return(self.dataframe[key])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQWw2jFQj90a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Element():\n",
        "    \"\"\"\n",
        "    One Snapshot contains many Elements  which represents graphs or tables\n",
        "    \n",
        "    Args:\n",
        "        parameters (dict): Dict containing all the parameters\n",
        "        sd (SnapshotData): The SnapshotData from which to build the element\n",
        "\n",
        "    Attributs:\n",
        "        parameters (dict{\n",
        "            type (string): The type of the Element in [\"graph\", \"table\"]\n",
        "            root_snapshot_data (SnapshotData): The SnapshotData from which the Element data will come (Default value: sd)\n",
        "            format_parameters (string): the way to format the data (See documentation for more information)\n",
        "            title (string): The title of the element (Default value: None)\n",
        "            subtitle (string): The subtitle of the element (Default value: None)\n",
        "            description (string): The descritpion of the element (Default value: None)\n",
        "            attachment_name (string): For tables only, the name of the email attached file (Default value: None)\n",
        "        })\n",
        "    \"\"\"\n",
        "    def __init__(self, sd, parameters):\n",
        "        default_parameters = {\n",
        "            \"type\":\"mandatory\",\n",
        "            \"root_snapshot_data\":sd,\n",
        "            \"format_parameters\":{},\n",
        "            \"title\":\"\",\n",
        "            \"subtitle\":\"\",\n",
        "            \"description\":\"\",\n",
        "            \"attachment_name\":\"\",\n",
        "            \"icon\":\"\",\n",
        "            \"template_name\":\"element\"\n",
        "        }\n",
        "        self.parameters = check_parameters(cp.deepcopy(parameters), default_parameters)\n",
        "        if len(self.parameters['icon']) > 0:\n",
        "            self.parameters[\"icon\"] = self.build_icon()\n",
        "            \n",
        "    def build_icon(self):\n",
        "        bucket = S3_BUCKET_CONF\n",
        "        icon_key = \"{s3_prefix}templates/{template_id}/img/icon/{icon_name}\".format(\n",
        "            s3_prefix=S3_PREFIX_CONF,\n",
        "            template_id=self.get_parameter(\"root_snapshot_data\").snapshot.email.get_parameter(\"template_id\"),\n",
        "            template_name=self.get_parameter(\"template_name\"),\n",
        "            icon_name=self.get_parameter(\"icon\")\n",
        "        )\n",
        "\n",
        "        icon_url = get_s3_url(\n",
        "            bucket=bucket,\n",
        "            key=icon_key\n",
        "        )\n",
        "\n",
        "        return(icon_url)\n",
        "        \n",
        "    def get_parameter(self, param, default=None):\n",
        "        if param in self.parameters[\"root_snapshot_data\"].snapshot.parameters:\n",
        "            return(self.parameters[\"root_snapshot_data\"].snapshot.parameters[param])\n",
        "        elif param in self.parameters[\"root_snapshot_data\"].parameters:\n",
        "            return(self.parameters[\"root_snapshot_data\"].parameters[param])\n",
        "        elif param in self.parameters:\n",
        "            return(self.parameters[param])\n",
        "        else:\n",
        "            return(default)\n",
        "        \n",
        "class Seller():\n",
        "    \"\"\"\n",
        "    Adomik Sellers to which the email(s) will be sent\n",
        "    \n",
        "    Args:\n",
        "        seller_id (string): The platform id of the seller\n",
        "        tam (string): The Technical Account Manager in charge of the account \n",
        "            in [\"carolyn\", \"louise\", \"rachel\", \"laura\", \"elly\", arnaud\", \"sho\"]\n",
        "        seller_name (string): The seller name (Default value =  seller_id)\n",
        "        product (string): The product of the seller in [\"display\", \"inapp\", \"game\", \"AMP\"] (Default value = \"display\")\n",
        "        email (list): The list of emails to which the snapshot will be sent (Default value = [])\n",
        "        custom_filter (string): A specific filter for the seller (See documentation for more information) (Default value = \"\")\n",
        "        attach_file (boolean): If True, the tables will be attached as xlsx file to the email (Default value = True)\n",
        "        send_mail (boolean): If False, the email won't be sent to this seller (Default value = True)\n",
        "\n",
        "    Attributs:\n",
        "        seller_id (string): The platform id of the seller\n",
        "        seller_name (string): The seller name\n",
        "        product (string): The product of the seller in [\"display\", \"inapp\", \"game\", \"AMP\"]\n",
        "        email (list): The list of emails to which the snapshot will be sent\n",
        "        custom_filter (string): A specific filter for the seller (See documentation for more information)\n",
        "        attach_file (boolean): If True, the tables will be attached as xlsx file to the email\n",
        "        tam (string): The Technical Account Manager in charge of the account \n",
        "            in [\"carolyn\", \"louise\", \"rachel\", \"laura\", \"elly\", arnaud\", \"sho\"]\n",
        "        send_mail (boolean): If False, the email won't be sent to this seller\n",
        "    \"\"\"\n",
        "    def __init__(self, seller_id, tam, seller_name=\"\", product=\"display\", email=[], custom_filter=\"\", attach_file=True, send_mail=True):\n",
        "        self.seller_id = str(seller_id)\n",
        "        self.seller_name = seller_name if len(seller_name) > 0 else self.seller_id\n",
        "        self.product = product\n",
        "        self.email = email\n",
        "        self.custom_filter = custom_filter\n",
        "        self.attach_file = attach_file\n",
        "        self.tam = tam\n",
        "        self.send_mail = send_mail\n",
        "        \n",
        "class Column():\n",
        "    \"\"\"\n",
        "    One SnapshotData can contains many Columns\n",
        "\n",
        "    Args:\n",
        "        name (string): The name of the column\n",
        "        pretty_name (string): The formated name of the column that will be used for display\n",
        "        col_type (string): The type of the elements in the column\n",
        "    \"\"\"\n",
        "    def __init__(self, name, col_type=\"string\", pretty_name=\"\"):\n",
        "        self.name = name\n",
        "        self.pretty_name = pretty_name\n",
        "        self.col_type = col_type\n",
        "        \n",
        "class Product():\n",
        "    \"\"\"\n",
        "    Different PRICE Products\n",
        "\n",
        "    Args:\n",
        "        name (string): The name of the column\n",
        "        pretty_name (string): The formated name of the column that will be used for display\n",
        "        col_type (string): The type of the elements in the column\n",
        "    \"\"\"\n",
        "    def __init__(self, name, channel=\"\", pretty_name=\"\"):\n",
        "        self.name = name\n",
        "        self.channel = get_products_channels(name) if channel == \"\" else channel\n",
        "        self.pretty_name = pretty_name\n",
        "        \n",
        "class Program():\n",
        "    \"\"\"\n",
        "    Different PRICE Products\n",
        "\n",
        "    Args:\n",
        "        name (string): The name of the column\n",
        "        pretty_name (string): The formated name of the column that will be used for display\n",
        "        col_type (string): The type of the elements in the column\n",
        "    \"\"\"\n",
        "    def __init__(self, program_id, name, first_version_id, first_version_date, pretty_name=\"\"):\n",
        "        self.program_id = program_id\n",
        "        self.name = name\n",
        "        self.first_version_id = first_version_id\n",
        "        self.first_version_date = first_version_date\n",
        "        self.pretty_name = pretty_name if pretty_name != \"\" else name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRZy9AbFj90c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Graph(Element):\n",
        "    \"\"\"\n",
        "    A Graph is an Element with the type \"graph\".\n",
        "    \n",
        "    Args:\n",
        "        parameters (dict): Dict containing all the parameters\n",
        "        sd (SnapshotData): The SnapshotData from which to build the element\n",
        "\n",
        "    Attributs:\n",
        "        parameters (dict{\n",
        "            kind (string): The kind of graph (See matplotlib.plot() documentation for more informations) (Default value = \"line\")\n",
        "            horizontal (boolean): If True, the graph will be displayed horizontaly (Default value = False)\n",
        "            linewidth (float): the width of the line to draw (Default value = 2.2)\n",
        "            colors (list[string]): A list of colors that will be used, hexadecimal format \n",
        "                (Default value = [\"#29AAE1\", \"#7DCBF2\", \"#599AD3\", \"#A4DAF5\", \"#40A3A3\", \"#D3F1F3\", \"#F7C32D\", \"#FCAF47\", \"#EA433B\"])\n",
        "            figsize (tuple(int)): The size of the figure (Default value = (16, 8))\n",
        "            rotation (int): The rotation of the x ticks of the graph (Default value = 0)\n",
        "            stacked (boolean): If True, the different curved will be stacked (Default value = False)\n",
        "            alpha (float): the opacity of the area graph (between 0 and 1) (Default value = 1)\n",
        "        })\n",
        "        snapshot_data (SnapshotData): The SnapshotData formated with the graph requirements\n",
        "        element_df (DataFrame): The DataFrame ready to be graphed\n",
        "        timestamp (string): The timestamp used when saving the graph\n",
        "    \"\"\"\n",
        "    def __init__(self, sd, parameters):\n",
        "        Element.__init__(self, sd, parameters)\n",
        "        adomik_colors = [\"#29AAE1\", \"#7DCBF2\", \"#599AD3\", \"#A4DAF5\", \"#40A3A3\", \"#D3F1F3\", \"#F7C32D\", \"#FCAF47\", \"#EA433B\"]\n",
        "        default_parameters = {\n",
        "            \"kind\":\"line\",\n",
        "            \"horizontal\":False,\n",
        "            \"linewidth\":2.2,\n",
        "            \"colors\":adomik_colors,\n",
        "            \"figsize\":(16, 8),\n",
        "            \"rotation\":0,\n",
        "            \"stacked\":False, \n",
        "            \"alpha\":1,\n",
        "            \"fontsize\":17\n",
        "        }\n",
        "        self.graph_parameters = check_parameters(self.parameters, default_parameters)\n",
        "        self.build_content()\n",
        "        \n",
        "    def build_content(self):\n",
        "        \"\"\"\n",
        "        Build the graph from the SnapshotData\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        new_aggregator = AggregatorEngine(\n",
        "            snapshot=self.parameters[\"root_snapshot_data\"].snapshot,\n",
        "            parameters=self.get_parameter(\"format_parameters\")\n",
        "        )\n",
        "        self.snapshot_data = self.parameters[\"root_snapshot_data\"].aggregate(agg_engine=new_aggregator)\n",
        "        self.element_df = self.snapshot_data.get_graphable_df(\n",
        "            pretty_named=True,\n",
        "            set_ind=self.parameters[\"format_parameters\"],\n",
        "            columns_renamer=self.get_parameter(\"columns_renamer\", default={})\n",
        "        )\n",
        "        extra_params = {}\n",
        "        if self.graph_parameters[\"kind\"] == \"line\":\n",
        "            extra_params[\"marker\"] = \"o\"\n",
        "        elif self.graph_parameters[\"kind\"] in [\"bar\", \"hbar\"]:\n",
        "            bar_width = 0.2\n",
        "            nb_cols = len(self.element_df.columns)\n",
        "            total_width = bar_width*nb_cols\n",
        "            extra_params[\"width\"] = total_width\n",
        "            \n",
        "        plt.figure()   \n",
        "        ax = self.element_df.plot(\n",
        "            kind=self.graph_parameters[\"kind\"],\n",
        "            stacked=self.graph_parameters[\"stacked\"],\n",
        "            figsize=self.graph_parameters[\"figsize\"],\n",
        "            legend=True,\n",
        "            linewidth=self.graph_parameters[\"linewidth\"],\n",
        "            color=self.graph_parameters[\"colors\"],\n",
        "            alpha=self.graph_parameters[\"alpha\"],\n",
        "            fontsize=self.graph_parameters[\"fontsize\"],\n",
        "            **extra_params)\n",
        "        \n",
        "        ax = self.format_graph_axis(ax)\n",
        "        bucket_name,key,timestamp = self.get_figure_path()\n",
        "        img_data = io.BytesIO()\n",
        "        self.save_graph_to_s3(bucket=bucket_name, key=key, plt_graph=img_data)\n",
        "        plt.close('all')\n",
        "        self.timestamp = timestamp\n",
        "        self.parameters[\"element_graph\"] = \"<img src='{img_url}' alt='unloaded graph'/>\".format(img_url=self.get_download_url())\n",
        "    \n",
        "    def format_graph_axis(self, ax):\n",
        "        \"\"\"\n",
        "        Format the axis of the graph\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        if len(self.element_df.index) == 1 and self.graph_parameters[\"kind\"] in [\"bar\", \"hbar\"]:\n",
        "            x_ind = [-(nb_cols-1)*bar_width/2 + i*bar_width for i in range (nb_cols)]\n",
        "            x_labels = list(self.element_df.columns)\n",
        "            plt.xticks(x_ind, x_labels)\n",
        "        if self.snapshot_data.columns[list(self.snapshot_data.columns.keys())[0]].col_type == \"perc\":\n",
        "            formatter = tick.FuncFormatter(self.to_percent)\n",
        "            if self.graph_parameters[\"kind\"] in ['barh']:\n",
        "                plt.gca().xaxis.set_major_formatter(formatter)\n",
        "            else:\n",
        "                plt.gca().yaxis.set_major_formatter(formatter)\n",
        "        elif self.snapshot_data.columns[list(self.snapshot_data.columns.keys())[0]].col_type == \"amount\":\n",
        "            formatter = tick.FuncFormatter(self.to_cur)\n",
        "            if self.graph_parameters[\"kind\"] in ['barh']:\n",
        "                plt.gca().xaxis.set_major_formatter(formatter)\n",
        "            else:\n",
        "                plt.gca().yaxis.set_major_formatter(formatter)\n",
        "        elif self.graph_parameters[\"kind\"] not in ['barh']:\n",
        "            formatter = tick.FuncFormatter(self.to_num)\n",
        "            plt.gca().yaxis.set_major_formatter(formatter)\n",
        "        if \"Date\" in self.element_df.columns:\n",
        "            plt.xticks(self.element_df.index, list(self.element_df[\"Date\"]))\n",
        "        plt.grid(b=True, linestyle=\":\")\n",
        "        if self.graph_parameters[\"kind\"] not in ['bar', 'barh']:\n",
        "            plt.gcf().subplots_adjust(left=0.05)\n",
        "        \n",
        "        plt.legend(prop={'size':self.graph_parameters[\"fontsize\"]})\n",
        "        ax.set_xlabel(ax.get_xlabel(), fontsize=self.graph_parameters[\"fontsize\"])\n",
        "        ax.set_ylabel(ax.get_ylabel(), fontsize=self.graph_parameters[\"fontsize\"])\n",
        "        plt.tight_layout()\n",
        "        return(ax)\n",
        "\n",
        "    def save_graph_to_s3(self, bucket, key, plt_graph):\n",
        "        \"\"\"\n",
        "        Saves the graph in s3\n",
        "\n",
        "        Args:\n",
        "            bucket (string): Name of the s3 bucket in which the graph will be saved\n",
        "            key (string): Name of the s3 key under which the graph will be saved \n",
        "            plt_graph (bytes): The graph stored in bytes\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        plt.savefig(plt_graph, format='png')\n",
        "        plt_graph.seek(0)\n",
        "        cli.client.put_object(Body=plt_graph, ContentType='image/png', Key=key, Bucket=bucket)\n",
        "        \n",
        "    def get_download_url(self):\n",
        "        \"\"\"\n",
        "        Retrieve the url of the graph to use it\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            (string) The url related to the graph\n",
        "        \"\"\"\n",
        "        bucket_name = S3_BUCKET_CONF\n",
        "        directory_path = S3_PREFIX_CONF+\"snapshots/{snapshot_type}/files\".format(\\\n",
        "            snapshot_type=self.parameters[\"root_snapshot_data\"].snapshot.parameters[\"snapshot_type\"]\\\n",
        "        )\n",
        "        try:\n",
        "            seller_id = self.parameters[\"root_snapshot_data\"].get_parameter(\"seller\").seller_id\n",
        "            ts = self.timestamp\n",
        "            key = \"{}/{}/{}.png\".format(directory_path, seller_id, ts)\n",
        "            s3_url = get_s3_url(bucket_name, key)\n",
        "            return(s3_url)\n",
        "        except e:\n",
        "            print(\"you have to build the table before retrieving the url\")\n",
        "            return\n",
        "\n",
        "    def get_figure_path(self):\n",
        "        \"\"\"\n",
        "        Return the name under which a figure will be saved in s3\n",
        "        \n",
        "        Args:\n",
        "            graph (Graph): The graph to save\n",
        "\n",
        "        Returns:\n",
        "            (string) The name under which to save the graph\n",
        "        \"\"\"\n",
        "        bucket_name = S3_BUCKET_CONF\n",
        "        directory_path = S3_PREFIX_CONF\n",
        "        graph_sd = self.parameters[\"root_snapshot_data\"]\n",
        "        timestamp = get_timestamp()\n",
        "        key = \"{directory_path}snapshots/{snapshot_type}/files/{seller}/{file_name}.png\".format(\\\n",
        "            directory_path = directory_path,\\\n",
        "            snapshot_type = graph_sd.snapshot.parameters[\"snapshot_type\"],\\\n",
        "            seller = graph_sd.get_parameter(\"seller\").seller_id,\\\n",
        "            file_name = timestamp\\\n",
        "        )\n",
        "        return([bucket_name, key, timestamp])\n",
        "        \n",
        "    def to_percent(self, y, position):\n",
        "        \"\"\"\n",
        "        Format a number to percent\n",
        "\n",
        "        Args:\n",
        "            y (int): The number to format\n",
        "\n",
        "        Returns:\n",
        "            (string) the formated number\n",
        "        \"\"\"\n",
        "        return '{:.2%}'.format(y)\n",
        "\n",
        "    def to_cur(self, y, position):\n",
        "        \"\"\"\n",
        "        Format a number to an amount with the currency\n",
        "\n",
        "        Args:\n",
        "            y (int): The number to format\n",
        "\n",
        "        Returns:\n",
        "            (string) the formated number\n",
        "        \"\"\"\n",
        "        return ((str(self.parameters[\"root_snapshot_data\"].get_parameter(\"currency\"))+'{:,.2f}'.format(y)))\n",
        "        \n",
        "    def to_num(self, y, position):\n",
        "        \"\"\"\n",
        "        Format a number to a number with thousand separator\n",
        "\n",
        "        Args:\n",
        "            y (int): The number to format\n",
        "\n",
        "        Returns:\n",
        "            (string) the formated number\n",
        "        \"\"\"\n",
        "        return '{:,.0f}'.format(y)\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BQ48Agfj90e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Table(Element):\n",
        "    \"\"\"\n",
        "    A Graph is an Element with the type \"graph\".\n",
        "    \n",
        "    Args:\n",
        "        parameters (dict): Dict containing all the parameters\n",
        "        sd (SnapshotData): The SnapshotData from which to build the element\n",
        "\n",
        "    Attributs:\n",
        "        snapshot_data (SnapshotData): The SnapshotData formated with the table requirements\n",
        "        element_df (DataFrame): The DataFrame formated to be displayed\n",
        "        html (string): The html code related to the table\n",
        "    \"\"\"\n",
        "    def __init__(self, sd, parameters):\n",
        "        Element.__init__(self, sd, parameters)\n",
        "        default_parameters = {\n",
        "            \"classes\":[]\n",
        "        }\n",
        "        self.table_parameters = check_parameters(self.parameters, default_parameters)\n",
        "        self.build_content()\n",
        "        \n",
        "    def build_content(self):\n",
        "        \"\"\"\n",
        "        Build the table from the SnapshotData\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        new_aggregator = AggregatorEngine(\n",
        "            snapshot=self.parameters[\"root_snapshot_data\"].snapshot,\n",
        "            parameters=self.get_parameter(\"format_parameters\")\n",
        "        )\n",
        "        self.snapshot_data = self.parameters[\"root_snapshot_data\"].aggregate(agg_engine=new_aggregator)\n",
        "\n",
        "        self.element_df = self.snapshot_data.get_graphable_df(\n",
        "            pretty_named=True,\n",
        "            columns_renamer=self.get_parameter(\"columns_renamer\", default={})).reset_index()\n",
        "        form_dict = self.get_formatters_dict()\n",
        "        self.element_df.style\n",
        "        self.parameters[\"element_table\"] = self.element_df\\\n",
        "        .style\\\n",
        "        .format(form_dict)\\\n",
        "        .set_table_attributes('class=\"element_table\"')\\\n",
        "        .hide_index()\\\n",
        "        .render()\n",
        "        \n",
        "    def get_formatters_dict(self):\n",
        "        \"\"\"\n",
        "        Build the dict containing all the column's formatters\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            (dict) For each column, assign a formatter\n",
        "        \"\"\"\n",
        "        formatters_dict = {}\n",
        "        cols = self.snapshot_data.columns\n",
        "        for col in cols:\n",
        "            if cols[col].col_type == \"int\":\n",
        "                formatters_dict[cols[col].pretty_name] = self.to_num\n",
        "            elif cols[col].col_type == \"perc\":\n",
        "                formatters_dict[cols[col].pretty_name+\" (%)\"] = self.to_perc\n",
        "            elif cols[col].col_type == \"amount\":\n",
        "                formatters_dict[cols[col].pretty_name+\" ({cur})\".format(cur=self.parameters[\"root_snapshot_data\"].get_parameter(\"currency\"))] = self.to_cur\n",
        "        return(formatters_dict)\n",
        "    \n",
        "    \n",
        "    def to_cur(self, y):\n",
        "        \"\"\"\n",
        "        Format a number to an amount with the currency\n",
        "\n",
        "        Args:\n",
        "            y (int): The number to format\n",
        "\n",
        "        Returns:\n",
        "            (string) the formated number\n",
        "        \"\"\"\n",
        "        return str(self.parameters[\"root_snapshot_data\"].get_parameter(\"currency\"))+'{:,.2f}'.format(y)\n",
        "        \n",
        "    def to_perc(self, y):\n",
        "        \"\"\"\n",
        "        Format a number to percent\n",
        "\n",
        "        Args:\n",
        "            y (int): The number to format\n",
        "\n",
        "        Returns:\n",
        "            (string) the formated number\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return '{0:.1%}'.format(y)\n",
        "        except:\n",
        "            return -1\n",
        "    \n",
        "    def to_num(self, y):\n",
        "        \"\"\"\n",
        "        Format a number to a number with thousand separator\n",
        "\n",
        "        Args:\n",
        "            y (int): The number to format\n",
        "\n",
        "        Returns:\n",
        "            (string) the formated number\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return \"{:,.0f}\".format(y)\n",
        "        except:\n",
        "            return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDV1A_sNj90g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Text(Element):\n",
        "    \"\"\"\n",
        "    A Text is an Element with the type \"text\".\n",
        "    \n",
        "    Args:\n",
        "        parameters (dict): Dict containing all the parameters\n",
        "        sd (SnapshotData): The SnapshotData from which to build the element\n",
        "\n",
        "    Attributs:\n",
        "        snapshot_data (SnapshotData): The SnapshotData formated with the text requirements\n",
        "        element_df (DataFrame): The DataFrame formated to be displayed\n",
        "        html (string): The html code related to the table\n",
        "    \"\"\"\n",
        "    def __init__(self, sd, parameters):\n",
        "        Element.__init__(self, sd, parameters)\n",
        "        default_parameters = {\n",
        "            \"text_html\":\"\",\n",
        "            \"red_and_green\":True,\n",
        "            \"fields_to_format\":{\n",
        "                \"uplift_cur\":{\n",
        "                    \"column\":\"uplift_cur\",\n",
        "                    \"row\":0\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        self.text_parameters = check_parameters(self.parameters, default_parameters)\n",
        "        self.build_content()\n",
        "        \n",
        "    def build_content(self):\n",
        "        \"\"\"\n",
        "        Build the text from the SnapshotData\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        new_aggregator = AggregatorEngine(\n",
        "            snapshot=self.parameters[\"root_snapshot_data\"].snapshot,\n",
        "            parameters=self.get_parameter(\"format_parameters\")\n",
        "        )\n",
        "        self.snapshot_data = self.parameters[\"root_snapshot_data\"].aggregate(agg_engine=new_aggregator)\n",
        "        \n",
        "        self.element_df = self.snapshot_data.get_graphable_df(\n",
        "            pretty_named=False,\n",
        "            set_ind=self.parameters[\"format_parameters\"],\n",
        "            columns_renamer=self.get_parameter(\"columns_renamer\", default={})\n",
        "        )\n",
        "        form_dict = self.get_formatters_dict()\n",
        "        self.parameters.update(form_dict)\n",
        "        \n",
        "    def get_formatters_dict(self):\n",
        "        \"\"\"\n",
        "        Build the dict containing all the column's formatters\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            (dict) For each column, assign a formatter\n",
        "        \"\"\"\n",
        "        formatters_dict = {}\n",
        "        cols = [tuple(e) for e in list(self.element_df.columns.values)]\n",
        "        rows = [tuple([e]) for e in list(self.element_df.index.values)]\n",
        "        for field in self.text_parameters[\"fields_to_format\"]:\n",
        "            field_ind = self.text_parameters[\"fields_to_format\"][field]\n",
        "\n",
        "            if tuple(field_ind[\"column\"]) in cols and tuple([field_ind[\"row\"]]) in rows:\n",
        "                val = self.element_df[tuple(field_ind[\"column\"])][field_ind[\"row\"]]\n",
        "            elif not tuple(field_ind[\"column\"]) in cols:\n",
        "                print(\"{col} not found in : {df_col}\".format(\n",
        "                    col=field_ind[\"column\"],\n",
        "                    df_col=str(list(self.element_df.columns))\n",
        "                ))\n",
        "                formatters_dict[field] = \"\"\n",
        "                continue\n",
        "            elif not tuple([field_ind[\"row\"]]) in rows:\n",
        "                print(\"{ind} not found in : {df_ind}\".format(\n",
        "                    ind=field_ind[\"row\"],\n",
        "                    df_ind=str(list(self.element_df.index))\n",
        "                ))\n",
        "                formatters_dict[field] = \"\"\n",
        "                continue\n",
        "            else:\n",
        "                formatters_dict[field] = \"\"\n",
        "                continue\n",
        "        \n",
        "            if self.snapshot_data.columns[field_ind[\"column\"][0]].col_type == \"int\":\n",
        "                formatters_dict[field] = self.to_num(val)\n",
        "            elif self.snapshot_data.columns[field_ind[\"column\"][0]].col_type == \"perc\":\n",
        "                formatters_dict[field] = self.to_perc(val)\n",
        "            elif self.snapshot_data.columns[field_ind[\"column\"][0]].col_type == \"amount\":\n",
        "                formatters_dict[field] = self.to_cur(val)\n",
        "        return(formatters_dict)\n",
        "    \n",
        "    \n",
        "    def to_cur(self, y):\n",
        "        \"\"\"\n",
        "        Format a number to an amount with the currency\n",
        "\n",
        "        Args:\n",
        "            y (int): The number to format\n",
        "\n",
        "        Returns:\n",
        "            (string) the formated number\n",
        "        \"\"\"\n",
        "        pref = \"\"\n",
        "        suf = \"\"\n",
        "        if self.text_parameters[\"red_and_green\"]:\n",
        "            pref = \"<span style='color: {color}'>\".format(color=\"#69a84f\" if y > 0 else \"#e02121\")\n",
        "            suf = \"</span>\"\n",
        "        return (pref+str(self.parameters[\"root_snapshot_data\"].get_parameter(\"currency\"))+'{:,.2f}'.format(y)+suf)\n",
        "        \n",
        "    def to_perc(self, y):\n",
        "        \"\"\"\n",
        "        Format a number to percent\n",
        "\n",
        "        Args:\n",
        "            y (int): The number to format\n",
        "\n",
        "        Returns:\n",
        "            (string) the formated number\n",
        "        \"\"\"\n",
        "        pref = \"\"\n",
        "        suf = \"\"\n",
        "        if self.text_parameters[\"red_and_green\"]:\n",
        "            pref = \"<span style='color: {color}'>\".format(color=\"#69a84f\" if y > 0 else \"#e02121\")\n",
        "            suf = \"</span>\"\n",
        "        try:\n",
        "            return(pref+'{0:.1%}'.format(y)+suf)\n",
        "        except:\n",
        "            return -1\n",
        "    \n",
        "    def to_num(self, y):\n",
        "        \"\"\"\n",
        "        Format a number to a number with thousand separator\n",
        "\n",
        "        Args:\n",
        "            y (int): The number to format\n",
        "\n",
        "        Returns:\n",
        "            (string) the formated number\n",
        "        \"\"\"\n",
        "        pref = \"\"\n",
        "        suf = \"\"\n",
        "        if self.text_parameters[\"red_and_green\"]:\n",
        "            pref = \"<span style='color: {color}'>\".format(color=\"#69a84f\" if y > 0 else \"#e02121\")\n",
        "            suf = \"</span>\"\n",
        "        try:\n",
        "            return (pref+\"{:,.0f}\".format(y)+suf)\n",
        "        except:\n",
        "            return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSGzFd-lj90j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Template():\n",
        "    \"\"\"\n",
        "    Template over which emails will be built\n",
        "    \n",
        "    Args:\n",
        "        name (string): The name of the template\n",
        "\n",
        "    Attributs:\n",
        "        email_html (string): The HTML code of the global email.\n",
        "        element_html (string): The HTML code of one element\n",
        "        css_stylesheet (string): The CSS stylesheet\n",
        "        formatters (dict): The elements that will need to be formatted in the HTML codes\n",
        "    \"\"\"\n",
        "    def __init__(self, template_id, template_name=\"\", element_html=\"\"):\n",
        "        self.template_id = template_id\n",
        "        self.template_elements = {}\n",
        "        if len(element_html) > 0:\n",
        "          self.template_html = element_html\n",
        "        elif isinstance(template_id, int):\n",
        "          self.template_html = get_pardot_template(self.template_id)\n",
        "          self.template_elements = self.create_template_elements_from_pardot_html()\n",
        "        else:\n",
        "          pass\n",
        "        \n",
        "        self.formatters = get_s3_file(\n",
        "            bucket=S3_BUCKET_CONF,\n",
        "            keys=S3_PREFIX_CONF+\"templates/{temp.template_id}/formatters.json\".format(temp=self)\n",
        "        )\n",
        "            \n",
        "            \n",
        "    def create_template_elements_from_pardot_html(self):\n",
        "      elements_dict = {}\n",
        "      elements_template_dict = get_elements_from_pardot_template(self.template_html)\n",
        "      for el, el_html in elements_template_dict.items():\n",
        "        elements_dict[el] = Template(\n",
        "            template_id=self.template_id,\n",
        "            template_name=el,\n",
        "            element_html=el_html\n",
        "        )\n",
        "      return(elements_dict)\n",
        "    \n",
        "    def format_template(self, obj):\n",
        "        \"\"\"\n",
        "        From all the formatters, format a template\n",
        "\n",
        "        Args:\n",
        "            template_type (string): Can be \"element\" or \"email\", describe which HTML code must be formatted\n",
        "\n",
        "        Returns:\n",
        "            (string) The formatted HTML code\n",
        "        \"\"\"\n",
        "        obj_formatters = {}\n",
        "        self.not_formatted_template_html = self.template_html\n",
        "        formatable_fields = get_formatable_fields(self.template_html)\n",
        "        for f in formatable_fields:\n",
        "            if f in self.formatters:\n",
        "                if obj.get_parameter(self.formatters[f]):\n",
        "                    if isinstance(obj.get_parameter(self.formatters[f]), str):\n",
        "                        obj_formatters[f] = format_to_universal_metrics(obj=obj, string=obj.get_parameter(self.formatters[f]))\n",
        "                    else:\n",
        "                        obj_formatters[f] = obj.get_parameter(self.formatters[f])\n",
        "                else:\n",
        "                    print(\"{f} not in {obj} parameters. Applying empty string\".format(f=str(f), obj=str(obj.__class__)))\n",
        "                    obj_formatters[f] = \"\"\n",
        "            else:\n",
        "                print(\"{f} not in formatters. Applying empty string.\".format(f=f))\n",
        "                obj_formatters[f] = \"\"\n",
        "        self.template_html = self.template_html.format(**obj_formatters)\n",
        "        \n",
        "    def replace_by_formatted_elements(self):\n",
        "        self.formatted_template_html = self.template_html\n",
        "        for el, temp_el in self.template_elements.items():\n",
        "            self.template_html = self.template_html.replace(temp_el.not_formatted_template_html, temp_el.template_html)\n",
        "            \n",
        "    def inline_css(self):\n",
        "      self.template_html = transform(self.template_html, disable_validation=True)\n",
        "      \n",
        "    def remove_css(self):\n",
        "      self.css_stylesheets = get_css_stylesheet(self.template_html)\n",
        "      for css_str in self.css_stylesheets:\n",
        "        self.template_html = self.template_html.replace(css_str, '')\n",
        "        \n",
        "    def restore_css(self):\n",
        "      splited_template_html = self.template_html.split(\"<!-- css_stylesheet --><!-- /css_stylesheet -->\")\n",
        "      self.template_html = splited_template_html[0] + ''.join(self.css_stylesheets) + splited_template_html[1]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MGyuvBgnIhd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7927eeeb-481f-4f5a-9a76-130137245108"
      },
      "source": [
        "\"abcdefg\".replace(\"a\", \"fds\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fdsbcdefg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47yGWZzmj90l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from email.mime.text import MIMEText\n",
        "from email.mime.image import MIMEImage\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.base import MIMEBase\n",
        "from email.mime.application import MIMEApplication\n",
        "from email import encoders\n",
        "from premailer import transform\n",
        "import smtplib, ssl\n",
        "from pypardot.client import PardotAPI\n",
        "\n",
        "class SMTPServer():\n",
        "    \"\"\"\n",
        "    SMTP server for any email server\n",
        "    \n",
        "    Args:\n",
        "        host (string): The host of the SMTP server\n",
        "        login (string): The login of the SMTP server\n",
        "        password (string): The password of the SMTP server\n",
        "        port (int): The port of the SMTP server (Default value = 25)\n",
        "\n",
        "    Attributs:\n",
        "        host (string): The host of the SMTP server\n",
        "        login (string): The login of the SMTP server\n",
        "        password (string): The password of the SMTP server\n",
        "        port (int): The port of the SMTP server (Default value = 25)\n",
        "    \"\"\"\n",
        "    def __init__(self, host, login, password, port=25):\n",
        "        \"\"\"conserve les paramètres d'un compte mail sur un serveur SMTP\"\"\"\n",
        "        self.host = host\n",
        "        self.port = port\n",
        "        self.login = login\n",
        "        self.password = password\n",
        "        \n",
        "class Email():\n",
        "    \"\"\"\n",
        "    Email containing the HTML code and all the informations that will be sent to clients\n",
        "    \n",
        "    Args:\n",
        "        parameters (dict): Dict containing all the parameters\n",
        "        snapshot (Snapshot): The snapshot related to this email\n",
        "\n",
        "    Attributs:\n",
        "        parameters ({\n",
        "            recipients (list[string]): email addresses list to whom will be sent the email\n",
        "            subject (string): Subject of the email\n",
        "            sender (string): Sender email address\n",
        "            reply_to (string): Address to whom the recipient(s) will answer\n",
        "            title (string): Title of the email\n",
        "            extra_infos (dict): Subject of the email\n",
        "            attachment_format (string): Format of the attached tables (csv or xlsx)\n",
        "            smtp_server (SMTPServer): Email server with login informations\n",
        "            attach_files (string): If True, the tables will be attached to the email\n",
        "            template (string): The name of the template to use. This template must exist in s3\n",
        "        })\n",
        "        snapshot (Snapshot): The snapshot related to this email\n",
        "        cpt_elements (int): The number of elements in the email\n",
        "        cpt_tables (int): The number of elements of the type graph in the email\n",
        "        content (string): HTML code of the email\n",
        "        template (Template): The template used by the email\n",
        "    \"\"\"\n",
        "    def __init__(self, snapshot, parameters={}):\n",
        "        default_parameters = {\n",
        "            \"recipients\":[],\n",
        "            \"subject\":\"[Adomik Snapshot] Report - \"+dateTimeToStr(datetime.datetime.today()),\n",
        "            \"sender\":\"report@adomik.com\",\n",
        "            \"reply_to\":\"report@adomik.com\",\n",
        "            \"title\":\"Report - \"+dateTimeToStr(datetime.datetime.today()),\n",
        "            \"attachment_format\":\"xlsx\",\n",
        "            \"smtp_server\":None,\n",
        "            \"attach_files\":True,\n",
        "            \"template_id\":\"mandatory\",\n",
        "            \"template_name\":\"email\"\n",
        "        }\n",
        "        self.parameters = check_parameters(parameters, default_parameters)\n",
        "        self.snapshot = snapshot\n",
        "        self.cpt_elements = 0\n",
        "        self.cpt_tables = 0\n",
        "        \n",
        "    def build_content(self):\n",
        "        \"\"\"\n",
        "        Build the HTML content from the template and the arguments to format\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.template = Template(\n",
        "            template_id=self.get_parameter(\"template_id\")\n",
        "        )\n",
        "        \n",
        "        for element in self.snapshot.elements:\n",
        "          el_template_name = element.get_parameter(\"template_name\")\n",
        "          if el_template_name in self.template.template_elements:\n",
        "            self.template.template_elements[el_template_name].format_template(\n",
        "              obj=element\n",
        "            )\n",
        "        \n",
        "        self.template.replace_by_formatted_elements()\n",
        "        self.template.remove_css()\n",
        "        self.template.format_template(\n",
        "              obj=self\n",
        "            )\n",
        "        self.template.restore_css()\n",
        "        self.template.inline_css()\n",
        "        email_content = self.template.template_html\n",
        "        \n",
        "        self.content = email_content\n",
        "\n",
        "    def build_message(self):\n",
        "        \"\"\"\n",
        "        Build the entire email with its parameters and attachments\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # Create the root message and fill in the from, to, and subject headers\n",
        "        self.msg = MIMEMultipart('related')\n",
        "        self.msg['Subject'] = format_to_universal_metrics(obj=self, string=self.parameters[\"subject\"])\n",
        "        self.msg['From'] = self.parameters[\"sender\"]\n",
        "        self.msg['To'] = \",\".join(self.parameters[\"recipients\"])\n",
        "        self.msg.add_header('reply-to', self.parameters[\"reply_to\"])\n",
        "        self.msg.preamble = 'This is a multi-part message in MIME format.'\n",
        "\n",
        "        # Encapsulate the plain and HTML versions of the message body in an\n",
        "        # 'alternative' part, so message agents can decide which they want to display\n",
        "        self.msgAlternative = MIMEMultipart('alternative')\n",
        "        self.msg.attach(self.msgAlternative)\n",
        "\n",
        "        # first add plain text\n",
        "        self.msg_text = MIMEText('The sender was not able to send you the email in an HTML format')\n",
        "        self.msgAlternative.attach(self.msg_text)\n",
        "        \n",
        "        if self.parameters['attach_files']:\n",
        "            for el in self.snapshot.elements:\n",
        "                if el.parameters['type'] == \"table\":\n",
        "                    filename = el.parameters['attachment_name']+\".\"+self.parameters[\"attachment_format\"] if not el.parameters['attachment_name'] is None else \"table\"+str(self.cpt_tables)+\".\"+self.parameters[\"attachment_format\"]\n",
        "                    attachment = MIMEApplication(export_excel(el.element_df))\n",
        "                    attachment['Content-Disposition'] = \"attachment; filename= %s\" % filename\n",
        "                    self.msg.attach(attachment)\n",
        "                    self.cpt_tables += 1\n",
        "\n",
        "        self.build_content()\n",
        "\n",
        "        self.msg_html = MIMEText(self.content, 'html', 'utf-8')\n",
        "        self.msgAlternative.attach(self.msg_html)\n",
        "        \n",
        "        print(\"!! email ready to be sent !!\")\n",
        "        \n",
        "        return(self.msg)\n",
        "\n",
        "    def send_email(self):\n",
        "        \"\"\"\n",
        "        Send the email through the chosen server\n",
        "\n",
        "        Args:\n",
        "            server (string): The selected server, can be \"adomik\" or \"localhost\"\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        server = self.get_parameter(\"server\")\n",
        "        attached_msg = self.build_message().as_string()\n",
        "        \n",
        "        if server == \"localhost\":\n",
        "            s = smtplib.SMTP('localhost')\n",
        "            s.sendmail(self.parameters[\"sender\"], self.parameters[\"recipients\"]+[\"henri@adomik.com\"], attached_msg)\n",
        "            \n",
        "        elif server == \"adomik\":\n",
        "            if self.parameters[\"smtp_server\"] is None:\n",
        "                s = start_gmail_server()\n",
        "            else:\n",
        "                s = self.parameters[\"smtp_server\"]\n",
        "            s.sendmail(self.parameters[\"sender\"], self.parameters[\"recipients\"]+[\"henri@adomik.com\"], attached_msg)\n",
        "            \n",
        "        elif server == \"pardot\":\n",
        "            send_mail_via_pardot(\n",
        "                html_content = self.content,\n",
        "                text_content=self.content,\n",
        "                from_email=self.get_parameter(\"sender\"),\n",
        "                replyto_email=self.get_parameter(\"replyto\"),\n",
        "                **self.parameters\n",
        "            )\n",
        "\n",
        "    def get_parameter(self, param, default=None):\n",
        "        if param in self.snapshot.parameters:\n",
        "            return(self.snapshot.parameters[param])\n",
        "        elif param in self.parameters:\n",
        "            return(self.parameters[param])\n",
        "        else:\n",
        "            return(default)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TlgdbiMj90o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def send_mail_via_pardot(**kwargs):\n",
        "    p = PardotAPI(\n",
        "        email=PARDOT_EMAIL,\n",
        "        password=PARDOT_PASSWD,\n",
        "        user_key=PARDOT_API_KEY\n",
        "    )\n",
        "    if p.authenticate():\n",
        "        if \"pardot_send_method\" in kwargs:\n",
        "            try:\n",
        "                if kwargs[\"pardot_send_method\"] == \"send_to_id\":\n",
        "                    p.emails.send_to_id(**kwargs)\n",
        "                elif kwargs[\"pardot_send_method\"] == \"send_to_lists\":\n",
        "                    p.emails.send_to_lists(**kwargs)\n",
        "                elif kwargs[\"pardot_send_method\"] == \"send_to_email\":\n",
        "                    p.emails.send_to_email(**kwargs)\n",
        "                else:\n",
        "                    print(\"Wrong pardot_send_method value.\")\n",
        "            except PardotAPIError as err:\n",
        "                raise err\n",
        "        else:\n",
        "            print(\"You must set the pardot_send_method to use Pardot\")\n",
        "    else:\n",
        "        print(\"Authentication problem, please try again later.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JXXMdzUj90r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PardotAPIError(Exception):\n",
        "    \"\"\"\n",
        "    Basic exception class for errors encountered in API post and get requests. Takes the json response and parses out\n",
        "    the error code and message.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, json_response):\n",
        "        self.response = json_response\n",
        "        self.err_code = json_response.get('@attributes').get('err_code')\n",
        "        self.message = str(json_response.get('err'))\n",
        "        if self.err_code is None:\n",
        "            self.err_code = 0\n",
        "            self.message = 'Unknown API error occurred'\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'Error #{err_code}: {message}'.format(err_code=self.err_code, message=self.message)\n",
        "\n",
        "\n",
        "class PardotAPIArgumentError(Exception):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfxZS7Ynj90t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(snapshot_type, gp, email_extra_conf={}, **kwargs):\n",
        "    now = time.time()\n",
        "    print(\"\"\"\n",
        "        /////////////////////////////////////////////////////////////////////////////////\\n\n",
        "        Start of the {snapshot_type} generation.\n",
        "        /////////////////////////////////////////////////////////////////////////////////\n",
        "    \"\"\".format(snapshot_type=snapshot_type))\n",
        "    \n",
        "    \n",
        "    #snapshot_parameters = cp.deepcopy(gp)\n",
        "    \n",
        "    snapshot_root_parameters = gp.get(\"snapshot_parameters\", {})\n",
        "    snapshot_root_parameters[\"snapshot_type\"] = snapshot_type\n",
        "    snapshot_root_parameters.update(kwargs)\n",
        "    print(\"debug\", \"ok3\")\n",
        "    \n",
        "    new_snapshot = Snapshot(snapshot_root_parameters)\n",
        "\n",
        "    email_conf = cp.deepcopy(gp[\"email_conf\"])\n",
        "    email_conf.update(email_extra_conf)\n",
        "    new_snapshot.build_email(email_conf)\n",
        "    print(\"debug\", \"ok4\")\n",
        "\n",
        "    for sd_conf in gp[\"content\"]:\n",
        "        print(\"debug\", \"ok5\")\n",
        "        new_snapshot_data = new_snapshot.build_snapshot_data(sd_conf[\"snapshot_data_parameters\"])\n",
        "        for el_conf in sd_conf[\"elements\"]:\n",
        "            print(\"Starting the creation of \"+el_conf[\"type\"]+\" : \"+el_conf[\"template_name\"]+\".\")\n",
        "            now = time.time()\n",
        "            el = new_snapshot_data.create_element(el_conf)\n",
        "            new_snapshot.add_element(el)\n",
        "            print(\"The \"+el_conf[\"type\"]+\" \"+el_conf[\"template_name\"]+\" has been loaded in : \"+str(time.time() - now)+\"s.\")\n",
        "            \n",
        "    if len(new_snapshot.elements) > 0:\n",
        "        new_snapshot.send()\n",
        "    else:\n",
        "        print(\"The Snapshot does not contain any data, no email sent.\")\n",
        "    \n",
        "    print(\"\"\"\n",
        "        /////////////////////////////////////////////////////////////////////////////////\\n\n",
        "        The {snapshot_type} has been sent.\\n\n",
        "        Execution time : {exec_time}s.\n",
        "        /////////////////////////////////////////////////////////////////////////////////\n",
        "    \"\"\".format(snapshot_type=snapshot_type, exec_time=str(time.time()-now))) \n",
        "    \n",
        "    return(new_snapshot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWUWO_u4j90u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAjqKEL2j90w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfD4VonCj90y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sell_newsletter = {\n",
        "   \"content\":[  \n",
        "      {  \n",
        "         \"elements\":[\n",
        "            {  \n",
        "               \"format_parameters\":{},\n",
        "               \"type\":\"text\",\"red_and_green\":True,\n",
        "                \"fields_to_format\":{\n",
        "                    \"cpm_evolution\":{\n",
        "                        \"column\":[\"cpm_resold_evolution\", \"05\"],\n",
        "                        \"row\":\"all\"\n",
        "                    },\n",
        "                    \"revenue_evolution\":{\n",
        "                        \"column\":[\"revenue_resold_evolution\", \"05\"],\n",
        "                        \"row\":\"all\"\n",
        "                    },\n",
        "                    \"imps_evolution\":{\n",
        "                        \"column\":[\"imps_resold_evolution\", \"05\"],\n",
        "                        \"row\":\"all\"\n",
        "                    },\n",
        "                    \"deal_evolution\":{\n",
        "                        \"column\":[\"revenue_deal_evolution\", \"05\"],\n",
        "                        \"row\":\"all\"\n",
        "                    },\n",
        "                    \"deal_coverage\":{\n",
        "                        \"column\":[\"revenue_resold_share_of_voice\", \"05\"],\n",
        "                        \"row\":\"Deal\"\n",
        "                    }\n",
        "                },\n",
        "                \"title\":\"1. Market figures for {edate:%B}*\",\n",
        "                \"template_name\":\"market_figures\"\n",
        "                \n",
        "            },\n",
        "         ],\n",
        "         \"snapshot_data_parameters\":{\n",
        "             \"pre_agg\":{\n",
        "               \"values\":{\n",
        "                  \"revenue_resold_evolution\":\"sum\",\n",
        "                  \"imps_resold_evolution\":\"sum\",\n",
        "                  \"cpm_resold_evolution\":\"sum\",\n",
        "                  \"revenue_deal_evolution\":\"sum\",\n",
        "                  \"revenue_resold_share_of_voice\":\"sum\",\n",
        "\n",
        "               },\n",
        "               \"groups\":[\n",
        "                  \"month\"\n",
        "               ],\n",
        "                \"keys\":[\n",
        "                    \"sales_channel\"\n",
        "                ],\n",
        "                \"custom_fields_conf\":{\n",
        "                    \"evolution\":{\n",
        "                        \"cpm_resold\":{\n",
        "                            \"compared_to\":\"04\"\n",
        "                        },\n",
        "                        \"revenue_resold\":{\n",
        "                            \"compared_to\":\"04\"\n",
        "                        },\n",
        "                        \"revenue_deal\":{\n",
        "                            \"compared_to\":\"04\"\n",
        "                        },\n",
        "                        \"imps_resold\":{\n",
        "                            \"compared_to\":\"04\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"share_of_voice\":{\n",
        "                        \"revenue_resold\":{\n",
        "                            \"axis\":0\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "                },\n",
        "                \"add_margins\":True\n",
        "            }\n",
        "         }\n",
        "      },\n",
        "      {  \n",
        "         \"elements\":[\n",
        "            {\n",
        "               \"format_parameters\":{\n",
        "                    \"pre_agg_filter\":\"month == '05'\",\n",
        "                    \"values\":{\"revenue_deal_share_of_voice\":\"sum\", \"revenue_resold\":\"sum\"},\n",
        "                    \"keys\":[\"brand\"],\n",
        "                    \"sort\":{\n",
        "                        \"by\":\"revenue_deal_share_of_voice\",\n",
        "                        \"ascending\":False\n",
        "                    },\n",
        "                    \"limit\":5,\n",
        "                    \"custom_fields_conf\":{\n",
        "                        \"share_of_voice\":{\n",
        "                            \"revenue_deal\":{\n",
        "                                \"axis\":0,\n",
        "                                \"compared_to\":\"revenue_resold\"\n",
        "                            }\n",
        "                        }\n",
        "                    },\n",
        "                   \"post_agg_filter\":\"brand != 'Unknown'\",\n",
        "                   \"select\":[\"revenue_deal_share_of_voice\"]\n",
        "                },\n",
        "                \"type\":\"table\",\n",
        "                \"template_name\":\"top_brands_deal\"\n",
        "            },\n",
        "             {\n",
        "               \"format_parameters\":{\n",
        "                    \"pre_agg_filter\":\"month == '05'\",\n",
        "                    \"values\":{\"revenue_deal_share_of_voice\":\"sum\", \"revenue_resold\":\"sum\"},\n",
        "                    \"keys\":[\"buyer\"],\n",
        "                    \"sort\":{\n",
        "                        \"by\":\"revenue_deal_share_of_voice\",\n",
        "                        \"ascending\":False\n",
        "                    },\n",
        "                    \"limit\":5,\n",
        "                    \"custom_fields_conf\":{\n",
        "                        \"share_of_voice\":{\n",
        "                            \"revenue_deal\":{\n",
        "                                \"axis\":0,\n",
        "                                \"compared_to\":\"revenue_resold\"\n",
        "                            }\n",
        "                        }\n",
        "                    },\n",
        "                   \"post_agg_filter\":\"buyer != 'Unknown'\",\n",
        "                    \"select\":[\"revenue_deal_share_of_voice\"]\n",
        "                },\n",
        "                \"type\":\"table\",\n",
        "                \"template_name\":\"top_buyers_deal\"\n",
        "            },\n",
        "         ],\n",
        "         \"snapshot_data_parameters\":{\n",
        "         }\n",
        "      }\n",
        "   ],\n",
        "   \"email_conf\":{\n",
        "      \"sender\":\"report@adomik.com\",\n",
        "      \"subject\":\"Test newsletter\",\n",
        "      \"template_id\":2881,\n",
        "      #\"pardot_send_method\":\"send_to_id\",\n",
        "      \"pardot_send_method\":\"send_to_lists\",\n",
        "      \"server\":\"pardot\",\n",
        "      #\"prospect_id\":5332427,\n",
        "      \"list_ids\":6789,\n",
        "      \"campaign_id\":4663,\n",
        "      \"from_name\":\"Adomik team\",\n",
        "      \"name\":\"sell_newsletter\",\n",
        "      \n",
        "   },\n",
        "   \"snapshot_parameters\":{\n",
        "      \"pattern\":\"sell_newsletter\",\n",
        "        \"columns_renamer\":{\n",
        "            \"buyer\":\"Buyer\",\n",
        "            \"revenue_resold_share_of_voice\":\"Market Share of Vol.\",\n",
        "            \"revenue_deal_share_of_voice\":\"Market Share of Vol.\"\n",
        "        }\n",
        "   }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8hPDgGwj90z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "dd97c9fa-f076-422d-ee8b-ed7de6ea0655"
      },
      "source": [
        "sell_data = get_sell_newsletter_logs(sdate, edate, program)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent: [--------------------------------------------------------------------------------------------------->] 100%Start df opitmization\n",
            "End df opitmization\n",
            "'load_sell_data'  635991.35 ms\n",
            "'lab_adk'  13918.20 ms\n",
            "df : 155867557\n",
            "df_lab_avt : 8764237\n",
            "df_lab_aps : 5998669\n",
            "df_tmp_avt : 596908333\n",
            "df_tmp_aps : 569823650\n",
            "'lab_ext'  14180.98 ms\n",
            "Start df opitmization\n",
            "End df opitmization\n",
            "'lab_adk'  3756.26 ms\n",
            "df : 871471955\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:981: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "df_lab_avt : 544582166\n",
            "df_lab_aps : 397000090\n",
            "df_tmp_avt : 4114972466\n",
            "df_tmp_aps : 3955883398\n",
            "'lab_ext'  129149.36 ms\n",
            "Start df opitmization\n",
            "End df opitmization\n",
            "'lab_adk'  14826.12 ms\n",
            "df : 34823647\n",
            "df_lab_avt : 2764\n",
            "df_lab_aps : 1854\n",
            "df_tmp_avt : 34823228\n",
            "df_tmp_aps : 34823228\n",
            "'lab_ext'  2965.77 ms\n",
            "Start df opitmization\n",
            "End df opitmization\n",
            "'label_sell_data'  232832.07 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tojVbJteEi7_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2b2fc15-eb52-4021-d911-217b8a6321d6"
      },
      "source": [
        "\n",
        "\n",
        "#nnndf = optimize_df(sell_datas)\n",
        "\n",
        "print(mem_usage(labeled_sell_data))\n",
        "#print(mem_usage(nnndf))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "924.54 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYAnVnSbl21u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        },
        "outputId": "1c1bb817-2849-4aec-c461-14f7dcc67055"
      },
      "source": [
        "sell_data.select_dtypes(include=['category'])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>brand_id</th>\n",
              "      <th>buyer_id</th>\n",
              "      <th>month</th>\n",
              "      <th>sales_channel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EasyJet</td>\n",
              "      <td>Adello Group</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Nissan</td>\n",
              "      <td>Adello Group</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VMware</td>\n",
              "      <td>iCrossing</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PetSmart</td>\n",
              "      <td>iCrossing</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bridgestone</td>\n",
              "      <td>iCrossing</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Unknown</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>First look</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Unknown</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Unknown</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Target</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>First look</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Target</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Chewy</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Cabela's</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Cabela's</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Ferrero</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Ferrero</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Macy's</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>First look</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Macy's</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Johnson and Johnson</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Kinder</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Office Depot</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>First look</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Office Depot</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Office Depot</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Allergan</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>us cellular</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>meijer</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>First look</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>meijer</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Kimberly-Clark Corporation</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Telephone and Data Systems</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>First look</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Telephone and Data Systems</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Telephone and Data Systems</td>\n",
              "      <td>OwnerIQ</td>\n",
              "      <td>04</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640003</th>\n",
              "      <td>Wyoming</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640004</th>\n",
              "      <td>Decorative Films</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640005</th>\n",
              "      <td>Nursing Center</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640006</th>\n",
              "      <td>BLue Hawaiian</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640007</th>\n",
              "      <td>GROVE CITY</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640008</th>\n",
              "      <td>SmartPak</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640009</th>\n",
              "      <td>OneBlade</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640010</th>\n",
              "      <td>Biofreeze Pain Reliever</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640011</th>\n",
              "      <td>Destination Hotels &amp; Resorts</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640012</th>\n",
              "      <td>Opal Sands Resort</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640013</th>\n",
              "      <td>Alaska USA</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640014</th>\n",
              "      <td>Wisconsin Department of Transportation</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640015</th>\n",
              "      <td>Bell State Bank &amp; Trust.</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640016</th>\n",
              "      <td>Brandywine Valley</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640017</th>\n",
              "      <td>Russell Lands on Lake Martin</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640018</th>\n",
              "      <td>Minor League Baseball</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640019</th>\n",
              "      <td>HearUSA</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640020</th>\n",
              "      <td>Bremer Bank</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640021</th>\n",
              "      <td>Medica</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640022</th>\n",
              "      <td>Slumberland Furniture</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640023</th>\n",
              "      <td>Tustin Cadillac</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640024</th>\n",
              "      <td>Conrad's Tire Express</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640025</th>\n",
              "      <td>Florida Rep</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640026</th>\n",
              "      <td>EVINE Live</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640027</th>\n",
              "      <td>Fred Martins Car Connection</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640028</th>\n",
              "      <td>Alabama Power</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640029</th>\n",
              "      <td>Baby + Co</td>\n",
              "      <td>rxb-Bi-5454</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640030</th>\n",
              "      <td>SelectBlinds</td>\n",
              "      <td>rxb-Bi-5464</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640031</th>\n",
              "      <td>1-800 CONTACTS, INC.</td>\n",
              "      <td>rxb-Bi-5464</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640032</th>\n",
              "      <td>dailysale.com</td>\n",
              "      <td>rxb-Bi-5464</td>\n",
              "      <td>05</td>\n",
              "      <td>Open auction</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15640033 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        brand_id  ... sales_channel\n",
              "0                                        EasyJet  ...  Open auction\n",
              "1                                         Nissan  ...  Open auction\n",
              "2                                         VMware  ...  Open auction\n",
              "3                                       PetSmart  ...  Open auction\n",
              "4                                    Bridgestone  ...  Open auction\n",
              "5                                        Unknown  ...    First look\n",
              "6                                        Unknown  ...       Unknown\n",
              "7                                        Unknown  ...  Open auction\n",
              "8                                         Target  ...    First look\n",
              "9                                         Target  ...  Open auction\n",
              "10                                         Chewy  ...  Open auction\n",
              "11                                      Cabela's  ...       Unknown\n",
              "12                                      Cabela's  ...  Open auction\n",
              "13                                       Ferrero  ...       Unknown\n",
              "14                                       Ferrero  ...  Open auction\n",
              "15                                        Macy's  ...    First look\n",
              "16                                        Macy's  ...  Open auction\n",
              "17                           Johnson and Johnson  ...  Open auction\n",
              "18                                        Kinder  ...  Open auction\n",
              "19                                  Office Depot  ...    First look\n",
              "20                                  Office Depot  ...       Unknown\n",
              "21                                  Office Depot  ...  Open auction\n",
              "22                                      Allergan  ...  Open auction\n",
              "23                                   us cellular  ...  Open auction\n",
              "24                                        meijer  ...    First look\n",
              "25                                        meijer  ...  Open auction\n",
              "26                    Kimberly-Clark Corporation  ...  Open auction\n",
              "27                    Telephone and Data Systems  ...    First look\n",
              "28                    Telephone and Data Systems  ...       Unknown\n",
              "29                    Telephone and Data Systems  ...  Open auction\n",
              "...                                          ...  ...           ...\n",
              "15640003                                 Wyoming  ...  Open auction\n",
              "15640004                        Decorative Films  ...  Open auction\n",
              "15640005                          Nursing Center  ...  Open auction\n",
              "15640006                           BLue Hawaiian  ...  Open auction\n",
              "15640007                              GROVE CITY  ...  Open auction\n",
              "15640008                                SmartPak  ...  Open auction\n",
              "15640009                                OneBlade  ...  Open auction\n",
              "15640010                 Biofreeze Pain Reliever  ...  Open auction\n",
              "15640011            Destination Hotels & Resorts  ...  Open auction\n",
              "15640012                       Opal Sands Resort  ...  Open auction\n",
              "15640013                              Alaska USA  ...  Open auction\n",
              "15640014  Wisconsin Department of Transportation  ...  Open auction\n",
              "15640015                Bell State Bank & Trust.  ...  Open auction\n",
              "15640016                       Brandywine Valley  ...  Open auction\n",
              "15640017            Russell Lands on Lake Martin  ...  Open auction\n",
              "15640018                   Minor League Baseball  ...  Open auction\n",
              "15640019                                 HearUSA  ...  Open auction\n",
              "15640020                             Bremer Bank  ...  Open auction\n",
              "15640021                                  Medica  ...  Open auction\n",
              "15640022                   Slumberland Furniture  ...  Open auction\n",
              "15640023                         Tustin Cadillac  ...  Open auction\n",
              "15640024                   Conrad's Tire Express  ...  Open auction\n",
              "15640025                             Florida Rep  ...  Open auction\n",
              "15640026                              EVINE Live  ...  Open auction\n",
              "15640027             Fred Martins Car Connection  ...  Open auction\n",
              "15640028                           Alabama Power  ...  Open auction\n",
              "15640029                               Baby + Co  ...  Open auction\n",
              "15640030                            SelectBlinds  ...  Open auction\n",
              "15640031                    1-800 CONTACTS, INC.  ...  Open auction\n",
              "15640032                           dailysale.com  ...  Open auction\n",
              "\n",
              "[15640033 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xls7ucuXUjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP28HahDoXif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvTWcj9fqce2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cjuwn7jrpt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8zBI6KlsJ_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlyLzZfbnZtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sdate = strToDateTime(\"2019-04-01\")\n",
        "edate = strToDateTime(\"2019-05-31\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D28UfVSZj901",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3638
        },
        "outputId": "36a27e98-30e2-4c73-a9f8-6a444d4a2014"
      },
      "source": [
        "print(\"Program started\")\n",
        "\n",
        "#serv = start_gmail_server()\n",
        "start_time = time.time()\n",
        "clients_sd = []\n",
        "sdate = strToDateTime(\"2019-04-01\")\n",
        "edate = strToDateTime(\"2019-05-31\")\n",
        "report_type = \"sell_newsletter\"\n",
        "\n",
        "\n",
        "program = get_program_from_name(\"FR\")\n",
        "\n",
        "general_parameters = sell_newsletter#get_s3_file(\"adomik-maths-preproduction\", \"henri/snapshot/snapshots/{rep_type}/{rep_type}.json\".format(rep_type=report_type))\n",
        "\n",
        "\n",
        "email_personal_conf = {\n",
        "    \"attach_files\":False\n",
        "}\n",
        "\n",
        "new_snap = main(\n",
        "    sdate=sdate,\n",
        "    edate=edate,\n",
        "    program=program,\n",
        "    snapshot_type=report_type,\n",
        "    gp=general_parameters,\n",
        "    email_extra_conf=email_personal_conf,\n",
        "    local=False\n",
        ")\n",
        "\n",
        "countries_sd = new_snap\n",
        "        \n",
        "print(\"Program ended in : {}s\".format(str(time.time()-start_time)))\n",
        "#serv.quit()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Program started\n",
            "\n",
            "        /////////////////////////////////////////////////////////////////////////////////\n",
            "\n",
            "        Start of the sell_newsletter generation.\n",
            "        /////////////////////////////////////////////////////////////////////////////////\n",
            "    \n",
            "debug ok3\n",
            "debug ok4\n",
            "debug ok5\n",
            "Percent: [--------------------------------------------------------------------------------------------------->] 100%Start df opitmization\n",
            "End df opitmization\n",
            "'load_sell_data'  561960.40 ms\n",
            "'lab_adk'  8927.61 ms\n",
            "df : 69021683\n",
            "df_lab_avt : 8764237\n",
            "df_lab_aps : 5998669\n",
            "df_tmp_avt : 245695799\n",
            "df_tmp_aps : 212901934\n",
            "'lab_ext'  7329.70 ms\n",
            "Start df opitmization\n",
            "End df opitmization\n",
            "'lab_adk'  3363.61 ms\n",
            "df : 492853250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:981: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "df_lab_avt : 544583116\n",
            "df_lab_aps : 397000760\n",
            "df_tmp_avt : 2253117682\n",
            "df_tmp_aps : 2124256954\n",
            "'lab_ext'  79627.63 ms\n",
            "Start df opitmization\n",
            "End df opitmization\n",
            "'lab_adk'  8984.49 ms\n",
            "df : 20763923\n",
            "df_lab_avt : 2764\n",
            "df_lab_aps : 1854\n",
            "df_tmp_avt : 20763372\n",
            "df_tmp_aps : 20763372\n",
            "'lab_ext'  2604.17 ms\n",
            "Start df opitmization\n",
            "End df opitmization\n",
            "'label_sell_data'  139927.11 ms\n",
            "Percent: [--------------------------------------------------------------------------------------------------->] 100%'rename_columns'  351.52 ms\n",
            "You must have Ad eCPM, Estimated revenue and Imps in columns. Not recalculating revenue\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'filter_df'  0.00 ms\n",
            "Start df unopitmization\n",
            "End df unopitmization\n",
            "Start df unopitmization\n",
            "End df unopitmization\n",
            "Start df unopitmization\n",
            "End df unopitmization\n",
            "'aggregate'  1184.34 ms\n",
            "'add_calculated_metrics'  13.31 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'set_compar'  0.17 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'filter_df'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'select'  0.07 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'sort'  0.06 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'limit'  0.48 ms\n",
            "debug ok2\n",
            "Starting the creation of text : market_figures.\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'filter_df'  0.00 ms\n",
            "'aggregate'  0.33 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'set_compar'  0.11 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'filter_df'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'select'  0.21 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'sort'  0.13 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'limit'  0.13 ms\n",
            "debug ok1\n",
            "Percent: [--------------------------------------------------------------------------------------------------->] 100%'rename_columns'  55.11 ms\n",
            "debug ok2\n",
            "The text market_figures has been loaded in : 0.0661623477935791s.\n",
            "debug ok5\n",
            "Percent: [--------------------------------------------------------------------------------------------------->] 100%'rename_columns'  332.37 ms\n",
            "You must have Ad eCPM, Estimated revenue and Imps in columns. Not recalculating revenue\n",
            "debug ok2\n",
            "Starting the creation of table : top_brands_deal.\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'filter_df'  645.14 ms\n",
            "Start df unopitmization\n",
            "End df unopitmization\n",
            "Start df unopitmization\n",
            "End df unopitmization\n",
            "'aggregate'  1232.33 ms\n",
            "'get_parameter'  0.01 ms\n",
            "'set_compar'  0.19 ms\n",
            "'get_parameter'  0.01 ms\n",
            "'get_parameter'  0.01 ms\n",
            "'filter_df'  80.03 ms\n",
            "'get_parameter'  0.01 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'select'  1.79 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'sort'  29.72 ms\n",
            "'get_parameter'  0.01 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'limit'  14.61 ms\n",
            "debug ok1\n",
            "Percent: [--------------------------------------------------------------------------------------------------->] 100%'rename_columns'  44.91 ms\n",
            "debug ok2\n",
            "The table top_brands_deal has been loaded in : 2.3576884269714355s.\n",
            "Starting the creation of table : top_buyers_deal.\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'filter_df'  650.28 ms\n",
            "Start df unopitmization\n",
            "End df unopitmization\n",
            "Start df unopitmization\n",
            "End df unopitmization\n",
            "'aggregate'  312.47 ms\n",
            "'get_parameter'  0.01 ms\n",
            "'set_compar'  0.16 ms\n",
            "'get_parameter'  0.01 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'filter_df'  4.52 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'select'  1.51 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'sort'  1.36 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'get_parameter'  0.00 ms\n",
            "'limit'  0.85 ms\n",
            "debug ok1\n",
            "Percent: [--------------------------------------------------------------------------------------------------->] 100%'rename_columns'  53.28 ms\n",
            "debug ok2\n",
            "The table top_buyers_deal has been loaded in : 1.2069339752197266s.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n",
            "WARNING\tProperty: Unknown Property name. [1:1: -ms-interpolation-mode]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "!! email ready to be sent !!\n",
            "\n",
            "        /////////////////////////////////////////////////////////////////////////////////\n",
            "\n",
            "        The sell_newsletter has been sent.\n",
            "\n",
            "        Execution time : 7.447907447814941s.\n",
            "        /////////////////////////////////////////////////////////////////////////////////\n",
            "    \n",
            "Program ended in : 715.4655318260193s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F34sxkFj93O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}